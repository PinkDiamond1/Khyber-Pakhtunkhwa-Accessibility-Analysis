{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join groups of rasters to large collections of points -- distributed computing version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to join values from a collection of large rasters (1.5+ GB each) to a large (1m+) collection of points. This leverages Dask + RioXarray to distribute the workload on a computing cluster. This notebook uses a Coiled cluster but note that Dask clusters can be set up on local machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The essential distributed computing theory at play in these analysis routines is to:\n",
    "1. Minimize input file sizes by removing extraneous values or columns\n",
    "2. Use spatial indices (bounding boxes, don't get too excited) to first divide up the points datasets amongst workers\n",
    "3. Correspondingly subdivide raster arrays and distribute each subdivision to the worker holding the corresponding points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask raster values > 360\n",
    "# Build this so we can loop over a collection of rasters\n",
    "# Do the aggregation operations per admin unit in Dask.DataFrames\n",
    "\n",
    "import dask\n",
    "import coiled\n",
    "from dask.distributed import Client, LocalCluster, Lock\n",
    "from dask.utils import SerializableLock\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "import rioxarray as rx\n",
    "import xarray as xr\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "def config_coiled_cluster(env_name=\"riox\"):\n",
    "    software_env_name = env_name\n",
    "    coiled.create_software_environment(\n",
    "        name=software_env_name,\n",
    "        #container=\"mrmaksimize/prefect-coiled-env:latest\",\n",
    "        pip=[\n",
    "            \"Fiona==1.8.19\",\n",
    "            \"rasterio==1.2.3\",\n",
    "            \"s3fs==2021.5.0\",\n",
    "            \"xarray==0.18.2\",\n",
    "            \"xarray-spatial==0.2.2\",\n",
    "            \"rioxarray==0.4.0\",\n",
    "            \"dask==2021.4.1\",\n",
    "            \"distributed==2021.4.1\"\n",
    "        ],\n",
    "        backend_options={\"region\": \"us-east-1\"})\n",
    "\n",
    "    # Create a cluster configuration named \"my-cluster-config\"\n",
    "    coiled.create_cluster_configuration(\n",
    "        name=f\"{software_env_name}-dev\",\n",
    "        scheduler_cpu=4,\n",
    "        #scheduler_memory=\"8 GiB\",\n",
    "        scheduler_memory=\"30 GiB\",\n",
    "        worker_cpu=4,\n",
    "        #worker_memory=\"8 GiB\",\n",
    "        worker_memory=\"30 GiB\",\n",
    "        software=f\"mrmaksimize/{software_env_name}\",\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input rasters\n",
    "\n",
    "# Insert the collection of rasters to be analyzed into a dictionary whose keys will serve as labels for outputs\n",
    "# Note this could be done programmatically from a folder using a regex for larger collections\n",
    "\n",
    "rasters = {\n",
    " 'dry_allroads_1993': 'dry_allroads_1993_20210516_WGS84_COG.tif',\n",
    " 'dry_allroads_2001': 'dry_allroads_2001_20210517_WGS84_COG.tif',\n",
    " 'dry_allroads_2011': 'dry_allroads_2011_20210516_WGS84_COG.tif',\n",
    " 'dry_allroads_2021': 'dry_allroads_2021_20210517_WGS84_COG.tif',\n",
    " 'dry_highways_1993': 'dry_highways_1993_20210516_WGS84_COG.tif',\n",
    " 'dry_highways_2001': 'dry_highways_2001_20210517_WGS84_COG.tif',\n",
    " 'dry_highways_2011': 'dry_highways_2011_20210516_WGS84_COG.tif',\n",
    " 'dry_highways_2021': 'dry_highways_2021_20210517_WGS84_COG.tif',\n",
    " 'dry_urban_centers_1993': 'dry_urban_centers_1993_20210516_WGS84_COG.tif',\n",
    " 'dry_urban_centers_2001': 'dry_urban_centers_2001_20210517_WGS84_COG.tif',\n",
    " 'dry_urban_centers_2011': 'dry_urban_centers_2011_20210516_WGS84_COG.tif',\n",
    " 'dry_urban_centers_2021': 'dry_urban_centers_2021_20210517_WGS84_COG.tif',\n",
    " 'msn_allroads_1993': 'msn_allroads_1993_20210516_WGS84_COG.tif',\n",
    " 'msn_allroads_2001': 'msn_allroads_2001_20210517_WGS84_COG.tif',\n",
    " 'msn_allroads_2011': 'msn_allroads_2011_20210517_WGS84_COG.tif',\n",
    " 'msn_allroads_2021': 'msn_allroads_2021_20210517_WGS84_COG.tif',\n",
    " 'msn_highways_1993': 'msn_highways_1993_20210516_WGS84_COG.tif',\n",
    " 'msn_highways_2001': 'msn_highways_2001_20210517_WGS84_COG.tif',\n",
    " 'msn_highways_2011': 'msn_highways_2011_20210517_WGS84_COG.tif',\n",
    " 'msn_highways_2021': 'msn_highways_2021_20210517_WGS84_COG.tif',\n",
    " 'msn_urban_centers_1993': 'msn_urban_centers_1993_20210516_WGS84_COG.tif',\n",
    " 'msn_urban_centers_2001': 'msn_urban_centers_2001_20210517_WGS84_COG.tif',\n",
    " 'msn_urban_centers_2011': 'msn_urban_centers_2011_20210517_WGS84_COG.tif',\n",
    " 'msn_urban_centers_2021': 'msn_urban_centers_2021_20210517_WGS84_COG.tif'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input points\n",
    "\n",
    "points_large = f\"{PATH_BASE}/NPL_WP_2020_19m_Pts_XY.csv\"\n",
    "points_small = f\"{PATH_BASE}/NPL_WP_2020_500k_Example_Pts_XY.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POINTS_URL = points_large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cutoff thresholds for the points and friction values. These help weed out model artefacts like fractionally populated areas or high travel times in the middle of rivers</br>Higher friction thresholds = more values included, including possibly extraneous ones. Higher population density thresholds = fewer points included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRICTION_THRESHOLD = 150\n",
    "DENSITY_THRESHOLD = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_BASE = \"s3://data-fully-public-bucket/nepal\"\n",
    "PATH_BASE_LOCAL = \"../data/nepal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path_mod = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster parameters -- if using AWS / Coiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"AKIAUEWT2Z5SGDVBLHEL\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"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\"\n",
    "#os.environ[\"DASK_COILED__USER\"] = \"MrMaksimize\" \n",
    "#os.environ['DASK_COILED__TOKEN'] = \"dd777d6d9b5c1f56f5ab105c767cbd4a08c1e78a\"\n",
    "#os.environ['DASK_COILED__SERVER'] = \"https://cloud.coiled.io\"\n",
    "\n",
    "STORAGE_OPTS = {\"secret\": os.environ.get(\"AWS_SECRET_ACCESS_KEY\"), \n",
    "                \"key\": os.environ.get(\"AWS_ACCESS_KEY_ID\")}\n",
    "\n",
    "CLUSTER = 'remote'\n",
    "#PATH_BASE = PATH_BASE_LOCAL # comment for S3\n",
    "COG_BASE = f\"{PATH_BASE}/cog\"\n",
    "#COG_BASE = f\"https://data-fully-public-bucket.s3.amazonaws.com/nepal/cog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating software environment...\n",
      "Found existing software environment build, returning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Found cluster configuration <span style=\"color: #008000; text-decoration-color: #008000\">'riox-dev'</span>, updating this configuration<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Found cluster configuration \u001b[32m'riox-dev'\u001b[0m, updating this configuration\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found software environment build\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tls://ec2-44-192-70-140.compute-1.amazonaws.com:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='http://ec2-44-192-70-140.compute-1.amazonaws.com:8787' target='_blank'>http://ec2-44-192-70-140.compute-1.amazonaws.com:8787</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tls://10.3.205.46:8786' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = None\n",
    "\n",
    "# Note that the number of workers and threads have direct impacts on the efficiency and cost of this operation.\n",
    "# Stated simply, more workers = more $$ for less computation time\n",
    "if CLUSTER == 'local':\n",
    "    cluster = LocalCluster(n_workers = 8, \n",
    "                       processes=True, \n",
    "                       threads_per_worker=8, \n",
    "                       scheduler_port=8786)\n",
    "    \n",
    "    client = Client(cluster)\n",
    "    \n",
    "else:\n",
    "    config_coiled_cluster(\"riox\")\n",
    "    cluster = coiled.Cluster(\n",
    "        name='riox-cluster',\n",
    "        configuration=f\"riox-dev\",\n",
    "        n_workers=4\n",
    "    )\n",
    "\n",
    "    client = Client(cluster) \n",
    "    #client.get_versions(packages=[\"xgboost\", \"numpy\"], check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the client to make sure it's working and get the dashboard/scheduler links\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tls://ec2-44-192-70-140.compute-1.amazonaws.com:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='http://ec2-44-192-70-140.compute-1.amazonaws.com:8787' target='_blank'>http://ec2-44-192-70-140.compute-1.amazonaws.com:8787</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tls://10.3.205.46:8786' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if needed\n",
    "client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define key functions we'll be using in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR MAKSIM -- are all of these functions needed or is only one relevant?\n",
    "# If the latter, shall we stash unused ones elsewhere for reference / safe keeping? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example workflow\n",
    "\"\"\"\n",
    "Extract pixel values from a dask-backed DataArray/Dataset at x, y coordinates given in a dask DataFrame.\n",
    "\n",
    "See the ``pixel_values_at_points`` docstring for usage.\n",
    "\n",
    "Example\n",
    "-------\n",
    ">>> import dask\n",
    ">>> import xarray as xr\n",
    ">>> data = xr.DataArray(\n",
    "...     da.random.random((256, 512), chunks=100),\n",
    "...     coords=[(\"y\", np.linspace(0, 1, 256)), (\"x\", np.linspace(0, 1, 512))],\n",
    "... )\n",
    ">>> df = dask.datasets.timeseries()\n",
    ">>> df_with_values = pixel_values_at_points(data, df)\n",
    ">>> df_with_values\n",
    "Dask DataFrame Structure:\n",
    "                   id    name        x        y pixel_values\n",
    "npartitions=17\n",
    "                int64  object  float64  float64      float64\n",
    "                  ...     ...      ...      ...          ...\n",
    "...               ...     ...      ...      ...          ...\n",
    "                  ...     ...      ...      ...          ...\n",
    "                  ...     ...      ...      ...          ...\n",
    "Dask Name: pixel-values, 103 tasks\n",
    ">>> df_with_values.head()\n",
    "                       id    name         x         y  pixel_values\n",
    "timestamp\n",
    "2000-01-01 00:00:35   980     Tim  0.054010  0.108094      0.704963\n",
    "2000-01-01 00:02:16  1045     Dan  0.009220  0.231619      0.904162\n",
    "2000-01-01 00:05:04  1020  Ursula  0.116082  0.048629      0.463596\n",
    "2000-01-01 00:06:31  1075  Ingrid  0.175014  0.383851      0.991749\n",
    "2000-01-01 00:07:35  1017   Kevin  0.067010  0.149642      0.661196\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import dask\n",
    "from dask.highlevelgraph import HighLevelGraph\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def _add_pixel_values_to_df(\n",
    "    pixels: np.ndarray,\n",
    "    y_coords: np.ndarray,\n",
    "    x_coords: np.ndarray,\n",
    "    partition: pd.DataFrame,\n",
    "    colname: str = \"value\",\n",
    ") -> pd.DataFrame:\n",
    "    # NOTE: coords must have already been chunked to align with `pixels`\n",
    "    # FOR MAKSIM -- how do they get \"already\" chunked?\n",
    "    arr = xr.DataArray(\n",
    "        pixels, coords=[(\"y\", y_coords.squeeze()), (\"x\", x_coords.squeeze())]\n",
    "    )\n",
    "\n",
    "    # Filter points that actually overlap with this chunk.\n",
    "    # Besides improving performance, this also saves us from having to do any merge\n",
    "    # step of the output dataframes: since raster chunks are non-overlapping\n",
    "    # spatially, the contents of each dataframe will be non-overlapping as well.\n",
    "\n",
    "    # minx, maxx, miny, maxy = x_coords[0], x_coords[-1], y_coords[-1], y_coords[0]\n",
    "    minx, miny, maxx, maxy = (  # noqa: F841\n",
    "        x_coords.min(),\n",
    "        y_coords.min(),\n",
    "        x_coords.max(),\n",
    "        y_coords.max(),\n",
    "    )\n",
    "\n",
    "    inbounds_df = partition.query(\"@minx < x < @maxx and @miny < y < @maxy\")\n",
    "    points_xr = xr.Dataset.from_dataframe(inbounds_df[[\"x\", \"y\"]])\n",
    "\n",
    "    # Select pixel values at points\n",
    "    # FOR MAKSIM -- so basically, we convert the points to an aligned raster dataset, select values that way, then return the values to a points dataset?\n",
    "    pixel_values = arr.sel(x=points_xr.x, y=points_xr.y, method=\"nearest\")\n",
    "    pixel_values_df = pixel_values.reset_coords(drop=True).to_dataframe(name=colname)\n",
    "    return pd.concat([inbounds_df, pixel_values_df], axis=\"columns\")\n",
    "\n",
    "\n",
    "def _pixel_values_at_points(\n",
    "    arr: da.Array,\n",
    "    ddf: dd.DataFrame,\n",
    "    y_coords: da.Array,\n",
    "    x_coords: da.Array,\n",
    "    colname: str = \"value\",\n",
    ") -> dd.DataFrame:\n",
    "    # TODO this would be easy\n",
    "    assert arr.ndim == 2, \"Multi-band not supported yet\"\n",
    "\n",
    "    # Turn each chunk of the raster into a DataFrame of pixel values extracted from that chunk.\n",
    "    px_values_df_chunks = arr.map_blocks(\n",
    "        _add_pixel_values_to_df,\n",
    "        # Expand coords arrays so they broadcast correctly\n",
    "        y_coords[:, None],\n",
    "        x_coords[None],\n",
    "        ddf,\n",
    "        colname=colname,\n",
    "        # give a fake meta since `_add_pixel_values_to_df` would fail on dummy data\n",
    "        meta=np.empty((0, 0)),\n",
    "    )\n",
    "    # NOTE: The cake is a lie! All of `px_values_df_chunks`'s metadata is wrong.\n",
    "    # Its chunks are DataFrames, not ndarrays; its shape and dtype are also\n",
    "    # not what `_meta` says.\n",
    "\n",
    "    # To turn the \"array\" of DataFrames into one dask DataFrame, we have to flatten it\n",
    "    # so its keys match the pattern of a DataFrame.\n",
    "    # Sadly we can't just `.ravel()`, because that will add actual `np.reshape` tasks to the graph,\n",
    "    # which will try to reshape the DataFrames (!!) to shapes that make no sense (!!!).\n",
    "\n",
    "    # So we throw up our hands and write a dask graph by hand that just renames from one key to another.\n",
    "    # TODO make this non-materialized (don't think it's blockwise-able though)\n",
    "\n",
    "    df_name = f\"pixel-values-{dask.base.tokenize(px_values_df_chunks)}\"\n",
    "    dsk = {\n",
    "        (df_name, i): key\n",
    "        for i, key in enumerate(dask.core.flatten(px_values_df_chunks.__dask_keys__()))\n",
    "    }\n",
    "    hlg = HighLevelGraph.from_collections(\n",
    "        df_name, dsk, dependencies=[px_values_df_chunks]\n",
    "    )\n",
    "\n",
    "    meta = pd.concat(\n",
    "        [ddf._meta, dd.utils.make_meta({colname: arr.dtype})], axis=\"columns\"\n",
    "    )\n",
    "    return dd.DataFrame(hlg, df_name, meta, (None,) * len(dsk))\n",
    "\n",
    "\n",
    "def pixel_values_at_points(\n",
    "    raster: Union[xr.DataArray, xr.Dataset], points: dd.DataFrame\n",
    ") -> dd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract pixel values from a DataArray or Dataset at coordinates given by a dask DataFrame\n",
    "\n",
    "    The ``points`` DataFrame should have columns ``x`` and ``y``, which should be in\n",
    "    the same coordinate reference system as the ``x`` and ``y`` coordinates on the\n",
    "    ``raster`` DataArray/Dataset.\n",
    "\n",
    "    The output DataFrame will contain the same data as ``points``, with one column added\n",
    "    per data variable in the Dataset (or just one column in the case of a DataArray).\n",
    "    The column(s) will have the same names as the data variables.\n",
    "\n",
    "    The output DataFrame will have one partition per chunk of the raster.\n",
    "    Each partition will contain only the points overlapping that chunk.\n",
    "    It will have the same index, but the divisions will be unknown.\n",
    "\n",
    "    Note that the entire ``points`` DataFrame will be loaded into memory; it cannot\n",
    "    be processed in parallel.\n",
    "    \"\"\"\n",
    "    # Turn x and y coordinates into dask arrays, so we can re-create the xarray\n",
    "    # object within our blockwise function. By using dask arrays instead of NumPy\n",
    "    # array literals, dask will split up the coordinates into chunks that align with the pixels.\n",
    "    # Ideally we could just use `DataArray.map_blocks` for all this, but 1) xarray's `map_blocks`\n",
    "    # doesn't support auxiliary non-xarray collections yet (i.e. ``points```), and 2) it generates\n",
    "    # a low-level graph instead of using a Blockwise layer.\n",
    "    try:\n",
    "        xs, ys = [\n",
    "            da.from_array(\n",
    "                raster.coords[coord].values,\n",
    "                chunks=(\n",
    "                    raster.chunks[raster.get_axis_num(coord)]\n",
    "                    if isinstance(raster, xr.DataArray)\n",
    "                    else raster.chunks[coord]\n",
    "                    # ^ NOTE: raises error if chunks are inconsistent\n",
    "                ),\n",
    "                inline_array=True,\n",
    "            )\n",
    "            for coord in [\"x\", \"y\"]\n",
    "        ]\n",
    "    except KeyError as e:\n",
    "        raise ValueError(f\"`rasters` is missing the coordinate '{e}'\")\n",
    "\n",
    "    if isinstance(raster, xr.DataArray):\n",
    "        return _pixel_values_at_points(\n",
    "            raster.data, points, ys, xs, colname=raster.name or \"value\"\n",
    "        )\n",
    "    else:\n",
    "        # TODO how do we merge all the DataFrames for non-overlapping arrays?\n",
    "        # They might not even be the same length, partitions won't correspond, etc.\n",
    "        # Ideally we'd have spatial partitioning to make the merge easy.\n",
    "        # But even without, I guess we need the index on each DataFrame to actually\n",
    "        # be identifying back to the original row so we can at least do a shuffle.\n",
    "        if len(set(arr.shape for arr in raster.data_vars.values())) != 1:\n",
    "            raise NotImplementedError(\n",
    "                \"Can't handle Datasets with variables of different shapes yet.\"\n",
    "            )\n",
    "        if len(set(arr.chunks for arr in raster.data_vars.values())) != 1:\n",
    "            raise NotImplementedError(\n",
    "                \"Can't handle Datasets with variables of different chunk patterns yet.\"\n",
    "            )\n",
    "\n",
    "        # NOTE: we use the full points dataframe for the first raster, and just the xy-coords\n",
    "        # for the subsequent ones, so we can concatenate all them column-wise without\n",
    "        # duplicating data.\n",
    "        points_justxy = points[[\"x\", \"y\"]]\n",
    "        dfs = [\n",
    "            _pixel_values_at_points(\n",
    "                arr.data, points if i == 0 else points_justxy, ys, xs, colname=name\n",
    "            )\n",
    "            for i, (name, arr) in enumerate(raster.data_vars.items())\n",
    "        ]\n",
    "\n",
    "        # NOTE: this is safe, since all the rasters have the same chunk pattern\n",
    "        # (and so we assume are spatially aligned; should actually check this),\n",
    "        # so the resulting DataFrames will have matching partitions.\n",
    "        return dd.concat(dfs, axis=\"columns\", ignore_unknown_divisions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting raster values to points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a dask dataframe (like a pandas DF but distribution-friendly) by ingesting a CSV file and trimming it down to only the essential columns to save space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VALUE_PTS</th>\n",
       "      <th>CBS_ID</th>\n",
       "      <th>CBS_Ward</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ix</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001882</td>\n",
       "      <td>60304</td>\n",
       "      <td>6030406</td>\n",
       "      <td>81.620833</td>\n",
       "      <td>30.445833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001878</td>\n",
       "      <td>60304</td>\n",
       "      <td>6030406</td>\n",
       "      <td>81.621666</td>\n",
       "      <td>30.445833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001938</td>\n",
       "      <td>60304</td>\n",
       "      <td>6030406</td>\n",
       "      <td>81.622500</td>\n",
       "      <td>30.445833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001996</td>\n",
       "      <td>60304</td>\n",
       "      <td>6030406</td>\n",
       "      <td>81.623333</td>\n",
       "      <td>30.445833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001992</td>\n",
       "      <td>60304</td>\n",
       "      <td>6030406</td>\n",
       "      <td>81.624166</td>\n",
       "      <td>30.445833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    VALUE_PTS  CBS_ID  CBS_Ward          x          y\n",
       "ix                                                   \n",
       "1    0.001882   60304   6030406  81.620833  30.445833\n",
       "2    0.001878   60304   6030406  81.621666  30.445833\n",
       "3    0.001938   60304   6030406  81.622500  30.445833\n",
       "4    0.001996   60304   6030406  81.623333  30.445833\n",
       "5    0.001992   60304   6030406  81.624166  30.445833"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = dd.read_csv(POINTS_URL, storage_options=STORAGE_OPTS,\n",
    "                     usecols = [\n",
    "                         0, #\"fid\",\n",
    "                         #1, #\"OBJECTID\",\n",
    "                         2, #\"VALUE\", \n",
    "                         #3, \"PALIKA\",\n",
    "                         #4, \"DISTRICT\",\n",
    "                         #5, \"GAPA_NAPA\",\n",
    "                         #6, \"GN_TYPE\",\n",
    "                         #7, \"PROVINCE\",\n",
    "                         8, #\"CBS_ID\",\n",
    "                         #9, \"HLCIT_ID\",\n",
    "                         #10,\"HLCIT_Ward\",\n",
    "                         11,#\"CBS_Ward\",\n",
    "                         12,#\"wgs84_x\",\n",
    "                         13,#\"wgs84_y\"\n",
    "                     ],\n",
    "                     header=0,\n",
    "                     names=[\n",
    "                         \"ix\",\n",
    "                         #\"OBJECTID\",\n",
    "                         \"VALUE_PTS\",\n",
    "                         #\"PALIKA\",\n",
    "                         #\"DISTRICT\",\n",
    "                         #\"GAPA_NAPA\",\n",
    "                         #\"GN_TYPE\",\n",
    "                         #\"PROVINCE\",\n",
    "                         \"CBS_ID\",\n",
    "                         #\"HLCIT_ID\",\n",
    "                         #\"HLCIT_Ward\",\n",
    "                         \"CBS_Ward\",\n",
    "                         \"x\",\n",
    "                         \"y\"\n",
    "                     ],\n",
    "                     dtype = {\n",
    "                         \"ix\": int, \n",
    "                         #\"OBJECTID\": int,\n",
    "                         \"VALUE_PTS\": float, \n",
    "                         #\"PALIKA\": str,\n",
    "                         #\"DISTRICT\": str,\n",
    "                         #\"GAPA_NAPA\":str, \n",
    "                         #\"GN_TYPE\": str, \n",
    "                         #\"PROVINCE\": str,\n",
    "                         \"CBS_ID\":str,  \n",
    "                         #\"HLCIT_ID\":str,\n",
    "                         #\"HLCIT_Ward\":str,\n",
    "                         \"CBS_Ward\": str,\n",
    "                         \"x\": float, \n",
    "                         \"y\": float \n",
    "                     },\n",
    "                     na_values = ' ',\n",
    "                     #blocksize='25mb'#'100mb'\n",
    "                ).set_index('ix', drop=True, sorted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some minor cleanup on that dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minor cleaning of dataframe\n",
    "points['CBS_Ward'] = points['CBS_Ward'].fillna('0')\n",
    "points['CBS_ID'] = points['CBS_ID'].fillna('0')\n",
    "points['CBS_Ward'] = points['CBS_Ward'].astype(float).astype(int)\n",
    "points['CBS_ID'] = points['CBS_ID'].astype(float).astype(int)\n",
    "points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove low pop densities\n",
    "points = points[points['VALUE_PTS'] > DENSITY_THRESHOLD]\n",
    "#points = points.repartition(npartitions=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR MAKSIM -- can we remove this?\n",
    "\n",
    "# Temp subset points\n",
    "#points_keep = [\n",
    "#    10609,\n",
    "#    31101,\n",
    "#    60703,\n",
    "#    40303,\n",
    "#    40301\n",
    "#]\n",
    "\n",
    "#points = points[points['CBS_ID'].isin(points_keep)]\n",
    "#out_path_mod = \"_subset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the collection of rasters as individual XArrays and save the collection into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persist raster: dry_allroads_1993 at s3://data-fully-public-bucket/nepal/cog/dry_allroads_1993_20210516_WGS84_COG.tif\n",
      "Persist raster: dry_allroads_2001 at s3://data-fully-public-bucket/nepal/cog/dry_allroads_2001_20210517_WGS84_COG.tif\n",
      "Persist raster: dry_allroads_2011 at s3://data-fully-public-bucket/nepal/cog/dry_allroads_2011_20210516_WGS84_COG.tif\n",
      "Persist raster: dry_allroads_2021 at s3://data-fully-public-bucket/nepal/cog/dry_allroads_2021_20210517_WGS84_COG.tif\n",
      "Persist raster: dry_highways_1993 at s3://data-fully-public-bucket/nepal/cog/dry_highways_1993_20210516_WGS84_COG.tif\n",
      "Persist raster: dry_highways_2001 at s3://data-fully-public-bucket/nepal/cog/dry_highways_2001_20210517_WGS84_COG.tif\n",
      "Persist raster: dry_highways_2011 at s3://data-fully-public-bucket/nepal/cog/dry_highways_2011_20210516_WGS84_COG.tif\n",
      "Persist raster: dry_highways_2021 at s3://data-fully-public-bucket/nepal/cog/dry_highways_2021_20210517_WGS84_COG.tif\n",
      "Persist raster: dry_urban_centers_1993 at s3://data-fully-public-bucket/nepal/cog/dry_urban_centers_1993_20210516_WGS84_COG.tif\n",
      "Persist raster: dry_urban_centers_2001 at s3://data-fully-public-bucket/nepal/cog/dry_urban_centers_2001_20210517_WGS84_COG.tif\n",
      "Persist raster: dry_urban_centers_2011 at s3://data-fully-public-bucket/nepal/cog/dry_urban_centers_2011_20210516_WGS84_COG.tif\n",
      "Persist raster: dry_urban_centers_2021 at s3://data-fully-public-bucket/nepal/cog/dry_urban_centers_2021_20210517_WGS84_COG.tif\n",
      "Persist raster: msn_allroads_1993 at s3://data-fully-public-bucket/nepal/cog/msn_allroads_1993_20210516_WGS84_COG.tif\n",
      "Persist raster: msn_allroads_2001 at s3://data-fully-public-bucket/nepal/cog/msn_allroads_2001_20210517_WGS84_COG.tif\n",
      "Persist raster: msn_allroads_2011 at s3://data-fully-public-bucket/nepal/cog/msn_allroads_2011_20210517_WGS84_COG.tif\n",
      "Persist raster: msn_allroads_2021 at s3://data-fully-public-bucket/nepal/cog/msn_allroads_2021_20210517_WGS84_COG.tif\n",
      "Persist raster: msn_highways_1993 at s3://data-fully-public-bucket/nepal/cog/msn_highways_1993_20210516_WGS84_COG.tif\n",
      "Persist raster: msn_highways_2001 at s3://data-fully-public-bucket/nepal/cog/msn_highways_2001_20210517_WGS84_COG.tif\n",
      "Persist raster: msn_highways_2011 at s3://data-fully-public-bucket/nepal/cog/msn_highways_2011_20210517_WGS84_COG.tif\n",
      "Persist raster: msn_highways_2021 at s3://data-fully-public-bucket/nepal/cog/msn_highways_2021_20210517_WGS84_COG.tif\n",
      "Persist raster: msn_urban_centers_2011 at s3://data-fully-public-bucket/nepal/cog/msn_urban_centers_2011_20210517_WGS84_COG.tif\n",
      "Persist raster: msn_urban_centers_1993 at s3://data-fully-public-bucket/nepal/cog/msn_urban_centers_1993_20210516_WGS84_COG.tif\n",
      "Persist raster: msn_urban_centers_2001 at s3://data-fully-public-bucket/nepal/cog/msn_urban_centers_2001_20210517_WGS84_COG.tif\n",
      "Persist raster: msn_urban_centers_2021 at s3://data-fully-public-bucket/nepal/cog/msn_urban_centers_2021_20210517_WGS84_COG.tif\n"
     ]
    }
   ],
   "source": [
    "# FOR MAKSIM -- why this number of chunks, what is 'auto', what is -1?\n",
    "# FOR MAKSIM - is it necessary to use XR instead of rasterio here?\n",
    "\n",
    "loaded_rasters = {}\n",
    "for key in rasters:\n",
    "    print(f\"Persist raster: {key} at {COG_BASE}/{rasters[key]}\")\n",
    "    raster = xr.open_rasterio(f\"{COG_BASE}/{rasters[key]}\", \n",
    "                                           chunks = (4, \"auto\", -1),\n",
    "                                           parse_coordinates=True)\n",
    "                                           #lock = True)\n",
    "    loaded_rasters[key] = raster\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a single XArray dataset with each of these rasters as one level\n",
    "# Weed out NoData values in the process\n",
    "\n",
    "rasters_ds = (\n",
    "    xr.Dataset(loaded_rasters)\n",
    "    .sel(band=1)\n",
    "    .map(lambda arr: arr.where(arr != arr.nodatavals[0]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the rasters and the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map pixel values to the points by relating the master Xarray dataset (not the individual TIF rasters) to the points\n",
    "df_pixels = pixel_values_at_points(rasters_ds, points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep all points where dry_allroads_2021_hrs < FRICTION_Threshold.\n",
    "# We use allroads arbitrarily here, any raster with extreme values could be used\n",
    "# FOR MAKSIM : why do we persist?\n",
    "df_pixels = df_pixels[df_pixels.dry_allroads_2021 < FRICTION_THRESHOLD]\n",
    "df_pixels = df_pixels.loc[:,~df_pixels.columns.duplicated()]\n",
    "df_pixels = df_pixels.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all you want to do is join a ton of raster values to points, without manipulating the point values, stop here and export your points.</br>If you want to manipulate the values joined to those points keep going -- we use the example of access values with small-scale Nepali administrative units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export code\n",
    "# df_pixels.to_csv(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate point values to enclosing admin units using pandas style dataframe manipulations on Dask Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Pops by CBS_ID\n",
    "adm3_pop = df_pixels.groupby('CBS_ID')['VALUE_PTS'].sum().to_frame(\"adm3_pop\")\n",
    "\n",
    "# Get Pops by CBS Ward\n",
    "adm4_pop = df_pixels.groupby('CBS_Ward')['VALUE_PTS'].sum().to_frame(\"adm4_pop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the Pops into Ref DF\n",
    "df_pixels = dd.merge(df_pixels, adm3_pop, how='left', left_on=\"CBS_ID\", right_index=True)\n",
    "df_pixels = dd.merge(df_pixels, adm4_pop, how = 'left', left_on=\"CBS_Ward\", right_index=True)\n",
    "\n",
    "#points = points.persist()\n",
    "# FOR MAKSIM : why do we persist?\n",
    "df_pixels = df_pixels.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the population weight of each pixel within its enclosing admin area -- e.g. 10 pixel population for a 100 population admin - 0.1 weight\n",
    "df_pixels['wt_adm_3'] = (df_pixels['VALUE_PTS'] / df_pixels['adm3_pop'])\n",
    "df_pixels['wt_adm_4'] = (df_pixels['VALUE_PTS'] / df_pixels['adm4_pop'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiply each pixel's access value by its weight. These will return miniscule values which will later summarize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm3_avg_cols = []\n",
    "adm4_avg_cols = []\n",
    "\n",
    "for rkey in rasters:\n",
    "    hrs_col = f\"{rkey}\"\n",
    "    avg_col_adm_3 = f\"{rkey}_avg_adm3\"\n",
    "    avg_col_adm_4 = f\"{rkey}_avg_adm4\"\n",
    "    df_pixels[avg_col_adm_3] = df_pixels[hrs_col] * df_pixels['wt_adm_3']\n",
    "    df_pixels[avg_col_adm_4] = df_pixels[hrs_col] * df_pixels['wt_adm_4']\n",
    "    \n",
    "    adm3_avg_cols.append(avg_col_adm_3)\n",
    "    adm4_avg_cols.append(avg_col_adm_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pixels = df_pixels.persist()\n",
    "#df_pixels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dry_allroads_1993_avt_adm3,dry_allroads_2001_avt_adm3,dry_allroads_2011_avt_adm3,dry_allroads_2021_avt_adm3,dry_highways_1993_avt_adm3,dry_highways_2001_avt_adm3,dry_highways_2011_avt_adm3,dry_highways_2021_avt_adm3,dry_urban_centers_1993_avt_adm3,dry_urban_centers_2001_avt_adm3,dry_urban_centers_2011_avt_adm3,dry_urban_centers_2021_avt_adm3,msn_allroads_1993_avt_adm3,msn_allroads_2001_avt_adm3,msn_allroads_2011_avt_adm3,msn_allroads_2021_avt_adm3,msn_highways_1993_avt_adm3,msn_highways_2001_avt_adm3,msn_highways_2011_avt_adm3,msn_highways_2021_avt_adm3,msn_urban_centers_2011_avt_adm3,msn_urban_centers_1993_avt_adm3,msn_urban_centers_2001_avt_adm3,msn_urban_centers_2021_avt_adm3\n",
      "dry_allroads_1993_avt_adm4,dry_allroads_2001_avt_adm4,dry_allroads_2011_avt_adm4,dry_allroads_2021_avt_adm4,dry_highways_1993_avt_adm4,dry_highways_2001_avt_adm4,dry_highways_2011_avt_adm4,dry_highways_2021_avt_adm4,dry_urban_centers_1993_avt_adm4,dry_urban_centers_2001_avt_adm4,dry_urban_centers_2011_avt_adm4,dry_urban_centers_2021_avt_adm4,msn_allroads_1993_avt_adm4,msn_allroads_2001_avt_adm4,msn_allroads_2011_avt_adm4,msn_allroads_2021_avt_adm4,msn_highways_1993_avt_adm4,msn_highways_2001_avt_adm4,msn_highways_2011_avt_adm4,msn_highways_2021_avt_adm4,msn_urban_centers_2011_avt_adm4,msn_urban_centers_1993_avt_adm4,msn_urban_centers_2001_avt_adm4,msn_urban_centers_2021_avt_adm4\n"
     ]
    }
   ],
   "source": [
    "# check out the results\n",
    "print(\",\".join(adm3_avg_cols))\n",
    "print(\",\".join(adm4_avg_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%time df_pixels.to_csv(f\"s3://data-fully-public-bucket/nepal/output/19m/df_pixels_raw{out_path_mod}.csv\", single_file = True, storage_options=STORAGE_OPTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the weighted access values for pixels belonging to each admin unit to yield the weighted average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgu = df_pixels.groupby(['CBS_ID'])[adm3_avg_cols].sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ward = df_pixels.groupby(['CBS_Ward'])[adm4_avg_cols].sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're done! Export the results, to AWS or your local machine as you please"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names for output CSV tables\n",
    "lgu_output = f\"s3://data-fully-public-bucket/nepal/output/19m/lgu{out_path_mod}.csv\"\n",
    "ward_output = f\"s3://data-fully-public-bucket/nepal/output/19m/ward{out_path_mod}.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://data-fully-public-bucket/nepal/output/19m/lgu.csv\n"
     ]
    }
   ],
   "source": [
    "print(lgu_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrmaksim/Code/rbanick_grav/venv/lib/python3.8/site-packages/dask/dataframe/io/csv.py:814: UserWarning: Appending data to a network storage system may not work.\n",
      "  warn(\"Appending data to a network storage system may not work.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 520 ms, sys: 34.3 ms, total: 555 ms\n",
      "Wall time: 3min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data-fully-public-bucket/nepal/output/19m/lgu.csv']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time lgu.to_csv(lgu_output, single_file = True, storage_options=STORAGE_OPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 137 ms, sys: 0 ns, total: 137 ms\n",
      "Wall time: 4.18 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data-fully-public-bucket/nepal/output/19m/ward.csv']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "asyncio.exceptions.CancelledError\n"
     ]
    }
   ],
   "source": [
    "%time ward.to_csv(ward_output, single_file = True, storage_options=STORAGE_OPTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unclear what's going on here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR MAKSIM -- is this leftover code from your experiments? Shall I shove it into a \"Scraps\" notebook? -- Robert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import xarray as xr\n",
    "data = xr.DataArray(\n",
    "    da.random.random((256, 512), chunks=100),\n",
    "    coords=[(\"y\", np.linspace(0, 1, 256)), (\"x\", np.linspace(0, 1, 512))],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dask.datasets.timeseries()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_values = pixel_values_at_points(data, df)\n",
    "df_with_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#points.head()\n",
    "chunk_list = []\n",
    "\n",
    "for chunk_df in points:\n",
    " \n",
    "    ind_x = xr.DataArray(chunk_df.x.values, dims=\"unique_index\")\n",
    "    ind_y = xr.DataArray(chunk_df.y.values, dims=\"unique_index\")\n",
    "    \n",
    "    # TODO - this should be parallel somehow\n",
    "    for rkey in loaded_rasters:\n",
    "        print(f\"Extracting from {rkey}\")\n",
    "        rds = loaded_rasters[rkey]\n",
    "        fill_val = rds.attrs['_FillValue']\n",
    "        result = rds.sel(band=1, x = ind_x, y = ind_y, method='nearest').values\n",
    "        chunk_df[f\"{rkey}_hrs\"] = result\n",
    "        chunk_df[chunk_df[f\"{rkey}_hrs\"] == fill_val] = None\n",
    "    \n",
    "    chunk_list.append(chunk_df)\n",
    "    \n",
    "p2 = pd.concat(chunk_list)\n",
    "p2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "for chunk_df in points:\n",
    "    print('e')\n",
    "    ind_x = xr.DataArray(chunk_df.x.values, dims=\"unique_index\")\n",
    "    ind_y = xr.DataArray(chunk_df.y.values, dims=\"unique_index\")\n",
    "    \n",
    "    # TODO - this should be parallel somehow\n",
    "    res = {}\n",
    "    for rkey in loaded_rasters:\n",
    "        print(f\"Extracting from {rkey}\")\n",
    "        rds = loaded_rasters[rkey]\n",
    "        res[rkey] = rds.sel(band=1, x = ind_x, y = ind_y, method='nearest')\n",
    "        \n",
    "    results.append(res)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = []\n",
    "for r in results:\n",
    "    proc.append(r['msn_allroads_2021'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = dask.compute(*proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_rasters = {}\n",
    "#url = \"https://data-fully-public-bucket.s3.amazonaws.com/nepal/cog/dry_allroads_2021_20210517_WGS84_COG.tif\"\n",
    "#url2 = \"https://data-fully-public-bucket.s3.amazonaws.com/nepal/cog/msn_allroads_2021_20210517_WGS84_COG.tif\"\n",
    "#loaded_rasters['dry_uncog'] = rx.open_rasterio(url, \n",
    "#                                       chunks = (4, \"auto\", -1),\n",
    "#                                       masked=False,\n",
    "#                                       parse_coordinates=True, \n",
    "#                                       lock = True).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_values_from_rasters(chunk_df, loaded_rasters, partition_info=None): \n",
    "    print(partition_info)\n",
    "    #If divisions are not known (for instance if the index is not sorted) then you will get None as the division.\n",
    "    ind_x = xr.DataArray(chunk_df.x.values, dims=\"unique_index\")\n",
    "    ind_y = xr.DataArray(chunk_df.y.values, dims=\"unique_index\")\n",
    "    \n",
    "    # TODO - this should be parallel somehow\n",
    "    for rkey in loaded_rasters:\n",
    "        print(f\"Extracting from {rkey}\")\n",
    "        rds = loaded_rasters[rkey]\n",
    "        fill_val = rds.attrs['_FillValue']\n",
    "        result = rds.sel(band=1, x = ind_x, y = ind_y, method='nearest').values\n",
    "        chunk_df[f\"{rkey}_hrs\"] = result\n",
    "        chunk_df[chunk_df[f\"{rkey}_hrs\"] == fill_val] = None\n",
    "    return chunk_df\n",
    "\n",
    "# Build output meta\n",
    "meta = {'VALUE': float, 'CBS_ID': int, 'CBS_Ward': int, 'x': float, 'y': float}\n",
    "for rkey in loaded_rasters:\n",
    "    meta[f\"{rkey}_hrs\"] = float\n",
    "\n",
    "\n",
    "# Promise the calculation\n",
    "# Adding loaded rasters per coiled team suggestion\n",
    "points = points.map_partitions(get_values_from_rasters, loaded_rasters=loaded_rasters,\n",
    "                                 meta=meta)\n",
    "points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgu = points.groupby(['CBS_ID'])[adm3_avg_cols].sum()\n",
    "lgu_output = \"s3://data-fully-public-bucket/nepal/output/500k/lgu.csv\"\n",
    "#lgu_output = \"../data/nepal/lgu_test.csv\"\n",
    "lgu.to_csv(lgu_output, single_file = True, storage_options=STORAGE_OPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ward = points.groupby(['CBS_Ward'])[adm4_avg_cols].sum()\n",
    "#ward_output = \"s3://data-fully-public-bucket/nepal/output/500k/lgu.csv\"\n",
    "#ward_output = \"../data/nepal/ward_test.csv\"\n",
    "#ward.to_csv(ward_output, single_file = True, storage_options=STORAGE_OPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#points.visualize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
