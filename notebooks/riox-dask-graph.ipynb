{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask raster values > 360\n",
    "# Build this so we can loop over a collection of rasters\n",
    "# Do the aggregation operations per admin unit in Dask.DataFrames\n",
    "\n",
    "import dask\n",
    "import coiled\n",
    "from dask.distributed import Client, LocalCluster, Lock\n",
    "from dask.utils import SerializableLock\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "import rioxarray as rx\n",
    "import xarray as xr\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "def config_coiled_cluster(env_name=\"riox\"):\n",
    "    software_env_name = env_name\n",
    "    coiled.create_software_environment(\n",
    "        name=software_env_name,\n",
    "        #container=\"mrmaksimize/prefect-coiled-env:latest\",\n",
    "        pip=[\n",
    "            \"Fiona==1.8.19\",\n",
    "            \"rasterio==1.2.3\",\n",
    "            \"s3fs==2021.5.0\",\n",
    "            \"xarray==0.18.2\",\n",
    "            \"xarray-spatial==0.2.2\",\n",
    "            \"rioxarray==0.4.0\",\n",
    "            \"dask==2021.4.1\",\n",
    "            \"distributed==2021.4.1\"\n",
    "        ],\n",
    "        backend_options={\"region\": \"us-east-1\"})\n",
    "\n",
    "    # Create a cluster configuration named \"my-cluster-config\"\n",
    "    coiled.create_cluster_configuration(\n",
    "        name=f\"{software_env_name}-dev\",\n",
    "        scheduler_cpu=4,\n",
    "        #scheduler_memory=\"8 GiB\",\n",
    "        scheduler_memory=\"30 GiB\",\n",
    "        worker_cpu=4,\n",
    "        #worker_memory=\"8 GiB\",\n",
    "        worker_memory=\"30 GiB\",\n",
    "        software=f\"mrmaksimize/{software_env_name}\",\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "rasters = {\n",
    " 'dry_allroads_1993': 'dry_allroads_1993_20210516_WGS84_COG.tif',\n",
    " 'dry_allroads_2001': 'dry_allroads_2001_20210517_WGS84_COG.tif',\n",
    " 'dry_allroads_2011': 'dry_allroads_2011_20210516_WGS84_COG.tif',\n",
    " 'dry_allroads_2021': 'dry_allroads_2021_20210517_WGS84_COG.tif',\n",
    " 'dry_highways_1993': 'dry_highways_1993_20210516_WGS84_COG.tif',\n",
    " 'dry_highways_2001': 'dry_highways_2001_20210517_WGS84_COG.tif',\n",
    " 'dry_highways_2011': 'dry_highways_2011_20210516_WGS84_COG.tif',\n",
    " 'dry_highways_2021': 'dry_highways_2021_20210517_WGS84_COG.tif',\n",
    " 'dry_urban_centers_1993': 'dry_urban_centers_1993_20210516_WGS84_COG.tif',\n",
    " 'dry_urban_centers_2001': 'dry_urban_centers_2001_20210517_WGS84_COG.tif',\n",
    " 'dry_urban_centers_2011': 'dry_urban_centers_2011_20210516_WGS84_COG.tif',\n",
    " 'dry_urban_centers_2021': 'dry_urban_centers_2021_20210517_WGS84_COG.tif',\n",
    " 'msn_allroads_1993': 'msn_allroads_1993_20210516_WGS84_COG.tif',\n",
    " 'msn_allroads_2001': 'msn_allroads_2001_20210517_WGS84_COG.tif',\n",
    " 'msn_allroads_2011': 'msn_allroads_2011_20210517_WGS84_COG.tif',\n",
    " 'msn_allroads_2021': 'msn_allroads_2021_20210517_WGS84_COG.tif',\n",
    " 'msn_highways_1993': 'msn_highways_1993_20210516_WGS84_COG.tif',\n",
    " 'msn_highways_2001': 'msn_highways_2001_20210517_WGS84_COG.tif',\n",
    " 'msn_highways_2011': 'msn_highways_2011_20210517_WGS84_COG.tif',\n",
    " 'msn_highways_2021': 'msn_highways_2021_20210517_WGS84_COG.tif',\n",
    " 'msn_urban_centers_1993': 'msn_urban_centers_1993_20210516_WGS84_COG.tif',\n",
    " 'msn_urban_centers_2001': 'msn_urban_centers_2001_20210517_WGS84_COG.tif',\n",
    " 'msn_urban_centers_2011': 'msn_urban_centers_2011_20210517_WGS84_COG.tif',\n",
    " 'msn_urban_centers_2021': 'msn_urban_centers_2021_20210517_WGS84_COG.tif'\n",
    "}\n",
    "\n",
    "FRICTION_THRESHOLD = 150\n",
    "DENSITY_THRESHOLD = 0.02\n",
    "\n",
    "PATH_BASE = \"s3://data-fully-public-bucket/nepal\"\n",
    "PATH_BASE_LOCAL = \"../data/nepal\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"AKIAUEWT2Z5SGDVBLHEL\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"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\"\n",
    "#os.environ[\"DASK_COILED__USER\"] = \"MrMaksimize\" \n",
    "#os.environ['DASK_COILED__TOKEN'] = \"dd777d6d9b5c1f56f5ab105c767cbd4a08c1e78a\"\n",
    "#os.environ['DASK_COILED__SERVER'] = \"https://cloud.coiled.io\"\n",
    "\n",
    "STORAGE_OPTS = {\"secret\": os.environ.get(\"AWS_SECRET_ACCESS_KEY\"), \n",
    "                \"key\": os.environ.get(\"AWS_ACCESS_KEY_ID\")}\n",
    "\n",
    "CLUSTER = 'remote'\n",
    "#PATH_BASE = PATH_BASE_LOCAL # comment for S3\n",
    "COG_BASE = f\"{PATH_BASE}/cog\"\n",
    "#COG_BASE = f\"https://data-fully-public-bucket.s3.amazonaws.com/nepal/cog\"\n",
    "\n",
    "\n",
    "points_large = f\"{PATH_BASE}/NPL_WP_2020_19m_Pts_XY.csv\"\n",
    "points_small = f\"{PATH_BASE}/NPL_WP_2020_500k_Example_Pts_XY.csv\"\n",
    "\n",
    "\n",
    "out_path_mod = \"\"\n",
    "\n",
    "POINTS_URL = points_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating software environment...\n",
      "Found existing software environment build, returning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Found cluster configuration <span style=\"color: #008000; text-decoration-color: #008000\">'riox-dev'</span>, updating this configuration<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Found cluster configuration \u001b[32m'riox-dev'\u001b[0m, updating this configuration\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found software environment build\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tls://ec2-44-192-70-140.compute-1.amazonaws.com:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='http://ec2-44-192-70-140.compute-1.amazonaws.com:8787' target='_blank'>http://ec2-44-192-70-140.compute-1.amazonaws.com:8787</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tls://10.3.205.46:8786' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "client = None\n",
    "\n",
    "if CLUSTER == 'local':\n",
    "    cluster = LocalCluster(n_workers = 8, \n",
    "                       processes=True, \n",
    "                       threads_per_worker=8, \n",
    "                       scheduler_port=8786)\n",
    "    \n",
    "    client = Client(cluster)\n",
    "    \n",
    "else:\n",
    "    config_coiled_cluster(\"riox\")\n",
    "    cluster = coiled.Cluster(\n",
    "        name='riox-cluster',\n",
    "        configuration=f\"riox-dev\",\n",
    "        n_workers=4\n",
    "    )\n",
    "\n",
    "    client = Client(cluster) \n",
    "    #client.get_versions(packages=[\"xgboost\", \"numpy\"], check=True)\n",
    "\n",
    "client\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tls://ec2-44-192-70-140.compute-1.amazonaws.com:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='http://ec2-44-192-70-140.compute-1.amazonaws.com:8787' target='_blank'>http://ec2-44-192-70-140.compute-1.amazonaws.com:8787</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tls://10.3.205.46:8786' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract pixel values from a dask-backed DataArray/Dataset at x, y coordinates given in a dask DataFrame.\n",
    "\n",
    "See the ``pixel_values_at_points`` docstring for usage.\n",
    "\n",
    "Example\n",
    "-------\n",
    ">>> import dask\n",
    ">>> import xarray as xr\n",
    ">>> data = xr.DataArray(\n",
    "...     da.random.random((256, 512), chunks=100),\n",
    "...     coords=[(\"y\", np.linspace(0, 1, 256)), (\"x\", np.linspace(0, 1, 512))],\n",
    "... )\n",
    ">>> df = dask.datasets.timeseries()\n",
    ">>> df_with_values = pixel_values_at_points(data, df)\n",
    ">>> df_with_values\n",
    "Dask DataFrame Structure:\n",
    "                   id    name        x        y pixel_values\n",
    "npartitions=17\n",
    "                int64  object  float64  float64      float64\n",
    "                  ...     ...      ...      ...          ...\n",
    "...               ...     ...      ...      ...          ...\n",
    "                  ...     ...      ...      ...          ...\n",
    "                  ...     ...      ...      ...          ...\n",
    "Dask Name: pixel-values, 103 tasks\n",
    ">>> df_with_values.head()\n",
    "                       id    name         x         y  pixel_values\n",
    "timestamp\n",
    "2000-01-01 00:00:35   980     Tim  0.054010  0.108094      0.704963\n",
    "2000-01-01 00:02:16  1045     Dan  0.009220  0.231619      0.904162\n",
    "2000-01-01 00:05:04  1020  Ursula  0.116082  0.048629      0.463596\n",
    "2000-01-01 00:06:31  1075  Ingrid  0.175014  0.383851      0.991749\n",
    "2000-01-01 00:07:35  1017   Kevin  0.067010  0.149642      0.661196\n",
    "\"\"\"\n",
    "\n",
    "from typing import Union\n",
    "import dask\n",
    "from dask.highlevelgraph import HighLevelGraph\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def _add_pixel_values_to_df(\n",
    "    pixels: np.ndarray,\n",
    "    y_coords: np.ndarray,\n",
    "    x_coords: np.ndarray,\n",
    "    partition: pd.DataFrame,\n",
    "    colname: str = \"value\",\n",
    ") -> pd.DataFrame:\n",
    "    # NOTE: coords must have already been chunked to align with `pixels`\n",
    "    arr = xr.DataArray(\n",
    "        pixels, coords=[(\"y\", y_coords.squeeze()), (\"x\", x_coords.squeeze())]\n",
    "    )\n",
    "\n",
    "    # Filter points that actually overlap with this chunk.\n",
    "    # Besides improving performance, this also saves us from having to do any merge\n",
    "    # step of the output dataframes: since raster chunks are non-overlapping\n",
    "    # spatially, the contents of each dataframe will be non-overlapping as well.\n",
    "\n",
    "    # minx, maxx, miny, maxy = x_coords[0], x_coords[-1], y_coords[-1], y_coords[0]\n",
    "    minx, miny, maxx, maxy = (  # noqa: F841\n",
    "        x_coords.min(),\n",
    "        y_coords.min(),\n",
    "        x_coords.max(),\n",
    "        y_coords.max(),\n",
    "    )\n",
    "\n",
    "    inbounds_df = partition.query(\"@minx < x < @maxx and @miny < y < @maxy\")\n",
    "    points_xr = xr.Dataset.from_dataframe(inbounds_df[[\"x\", \"y\"]])\n",
    "\n",
    "    # Select pixel values at points\n",
    "    pixel_values = arr.sel(x=points_xr.x, y=points_xr.y, method=\"nearest\")\n",
    "    pixel_values_df = pixel_values.reset_coords(drop=True).to_dataframe(name=colname)\n",
    "    return pd.concat([inbounds_df, pixel_values_df], axis=\"columns\")\n",
    "\n",
    "\n",
    "def _pixel_values_at_points(\n",
    "    arr: da.Array,\n",
    "    ddf: dd.DataFrame,\n",
    "    y_coords: da.Array,\n",
    "    x_coords: da.Array,\n",
    "    colname: str = \"value\",\n",
    ") -> dd.DataFrame:\n",
    "    # TODO this would be easy\n",
    "    assert arr.ndim == 2, \"Multi-band not supported yet\"\n",
    "\n",
    "    # Turn each chunk of the raster into a DataFrame of pixel values extracted from that chunk.\n",
    "    px_values_df_chunks = arr.map_blocks(\n",
    "        _add_pixel_values_to_df,\n",
    "        # Expand coords arrays so they broadcast correctly\n",
    "        y_coords[:, None],\n",
    "        x_coords[None],\n",
    "        ddf,\n",
    "        colname=colname,\n",
    "        # give a fake meta since `_add_pixel_values_to_df` would fail on dummy data\n",
    "        meta=np.empty((0, 0)),\n",
    "    )\n",
    "    # NOTE: The cake is a lie! All of `px_values_df_chunks`'s metadata is wrong.\n",
    "    # Its chunks are DataFrames, not ndarrays; its shape and dtype are also\n",
    "    # not what `_meta` says.\n",
    "\n",
    "    # To turn the \"array\" of DataFrames into one dask DataFrame, we have to flatten it\n",
    "    # so its keys match the pattern of a DataFrame.\n",
    "    # Sadly we can't just `.ravel()`, because that will add actual `np.reshape` tasks to the graph,\n",
    "    # which will try to reshape the DataFrames (!!) to shapes that make no sense (!!!).\n",
    "\n",
    "    # So we throw up our hands and write a dask graph by hand that just renames from one key to another.\n",
    "    # TODO make this non-materialized (don't think it's blockwise-able though)\n",
    "\n",
    "    df_name = f\"pixel-values-{dask.base.tokenize(px_values_df_chunks)}\"\n",
    "    dsk = {\n",
    "        (df_name, i): key\n",
    "        for i, key in enumerate(dask.core.flatten(px_values_df_chunks.__dask_keys__()))\n",
    "    }\n",
    "    hlg = HighLevelGraph.from_collections(\n",
    "        df_name, dsk, dependencies=[px_values_df_chunks]\n",
    "    )\n",
    "\n",
    "    meta = pd.concat(\n",
    "        [ddf._meta, dd.utils.make_meta({colname: arr.dtype})], axis=\"columns\"\n",
    "    )\n",
    "    return dd.DataFrame(hlg, df_name, meta, (None,) * len(dsk))\n",
    "\n",
    "\n",
    "def pixel_values_at_points(\n",
    "    raster: Union[xr.DataArray, xr.Dataset], points: dd.DataFrame\n",
    ") -> dd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract pixel values from a DataArray or Dataset at coordinates given by a dask DataFrame\n",
    "\n",
    "    The ``points`` DataFrame should have columns ``x`` and ``y``, which should be in\n",
    "    the same coordinate reference system as the ``x`` and ``y`` coordinates on the\n",
    "    ``raster`` DataArray/Dataset.\n",
    "\n",
    "    The output DataFrame will contain the same data as ``points``, with one column added\n",
    "    per data variable in the Dataset (or just one column in the case of a DataArray).\n",
    "    The column(s) will have the same names as the data variables.\n",
    "\n",
    "    The output DataFrame will have one partition per chunk of the raster.\n",
    "    Each partition will contain only the points overlapping that chunk.\n",
    "    It will have the same index, but the divisions will be unknown.\n",
    "\n",
    "    Note that the entire ``points`` DataFrame will be loaded into memory; it cannot\n",
    "    be processed in parallel.\n",
    "    \"\"\"\n",
    "    # Turn x and y coordinates into dask arrays, so we can re-create the xarray\n",
    "    # object within our blockwise function. By using dask arrays instead of NumPy\n",
    "    # array literals, dask will split up the coordinates into chunks that align with the pixels.\n",
    "    # Ideally we could just use `DataArray.map_blocks` for all this, but 1) xarray's `map_blocks`\n",
    "    # doesn't support auxiliary non-xarray collections yet (i.e. ``points```), and 2) it generates\n",
    "    # a low-level graph instead of using a Blockwise layer.\n",
    "    try:\n",
    "        xs, ys = [\n",
    "            da.from_array(\n",
    "                raster.coords[coord].values,\n",
    "                chunks=(\n",
    "                    raster.chunks[raster.get_axis_num(coord)]\n",
    "                    if isinstance(raster, xr.DataArray)\n",
    "                    else raster.chunks[coord]\n",
    "                    # ^ NOTE: raises error if chunks are inconsistent\n",
    "                ),\n",
    "                inline_array=True,\n",
    "            )\n",
    "            for coord in [\"x\", \"y\"]\n",
    "        ]\n",
    "    except KeyError as e:\n",
    "        raise ValueError(f\"`rasters` is missing the coordinate '{e}'\")\n",
    "\n",
    "    if isinstance(raster, xr.DataArray):\n",
    "        return _pixel_values_at_points(\n",
    "            raster.data, points, ys, xs, colname=raster.name or \"value\"\n",
    "        )\n",
    "    else:\n",
    "        # TODO how do we merge all the DataFrames for non-overlapping arrays?\n",
    "        # They might not even be the same length, partitions won't correspond, etc.\n",
    "        # Ideally we'd have spatial partitioning to make the merge easy.\n",
    "        # But even without, I guess we need the index on each DataFrame to actually\n",
    "        # be identifying back to the original row so we can at least do a shuffle.\n",
    "        if len(set(arr.shape for arr in raster.data_vars.values())) != 1:\n",
    "            raise NotImplementedError(\n",
    "                \"Can't handle Datasets with variables of different shapes yet.\"\n",
    "            )\n",
    "        if len(set(arr.chunks for arr in raster.data_vars.values())) != 1:\n",
    "            raise NotImplementedError(\n",
    "                \"Can't handle Datasets with variables of different chunk patterns yet.\"\n",
    "            )\n",
    "\n",
    "        # NOTE: we use the full points dataframe for the first raster, and just the xy-coords\n",
    "        # for the subsequent ones, so we can concatenate all them column-wise without\n",
    "        # duplicating data.\n",
    "        points_justxy = points[[\"x\", \"y\"]]\n",
    "        dfs = [\n",
    "            _pixel_values_at_points(\n",
    "                arr.data, points if i == 0 else points_justxy, ys, xs, colname=name\n",
    "            )\n",
    "            for i, (name, arr) in enumerate(raster.data_vars.items())\n",
    "        ]\n",
    "\n",
    "        # NOTE: this is safe, since all the rasters have the same chunk pattern\n",
    "        # (and so we assume are spatially aligned; should actually check this),\n",
    "        # so the resulting DataFrames will have matching partitions.\n",
    "        return dd.concat(dfs, axis=\"columns\", ignore_unknown_divisions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VALUE_PTS</th>\n",
       "      <th>CBS_ID</th>\n",
       "      <th>CBS_Ward</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ix</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001882</td>\n",
       "      <td>60304</td>\n",
       "      <td>6030406</td>\n",
       "      <td>81.620833</td>\n",
       "      <td>30.445833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001878</td>\n",
       "      <td>60304</td>\n",
       "      <td>6030406</td>\n",
       "      <td>81.621666</td>\n",
       "      <td>30.445833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001938</td>\n",
       "      <td>60304</td>\n",
       "      <td>6030406</td>\n",
       "      <td>81.622500</td>\n",
       "      <td>30.445833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001996</td>\n",
       "      <td>60304</td>\n",
       "      <td>6030406</td>\n",
       "      <td>81.623333</td>\n",
       "      <td>30.445833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001992</td>\n",
       "      <td>60304</td>\n",
       "      <td>6030406</td>\n",
       "      <td>81.624166</td>\n",
       "      <td>30.445833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    VALUE_PTS  CBS_ID  CBS_Ward          x          y\n",
       "ix                                                   \n",
       "1    0.001882   60304   6030406  81.620833  30.445833\n",
       "2    0.001878   60304   6030406  81.621666  30.445833\n",
       "3    0.001938   60304   6030406  81.622500  30.445833\n",
       "4    0.001996   60304   6030406  81.623333  30.445833\n",
       "5    0.001992   60304   6030406  81.624166  30.445833"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = dd.read_csv(POINTS_URL, storage_options=STORAGE_OPTS,\n",
    "                     usecols = [\n",
    "                         0, #\"fid\",\n",
    "                         #1, #\"OBJECTID\",\n",
    "                         2, #\"VALUE\", \n",
    "                         #3, \"PALIKA\",\n",
    "                         #4, \"DISTRICT\",\n",
    "                         #5, \"GAPA_NAPA\",\n",
    "                         #6, \"GN_TYPE\",\n",
    "                         #7, \"PROVINCE\",\n",
    "                         8, #\"CBS_ID\",\n",
    "                         #9, \"HLCIT_ID\",\n",
    "                         #10,\"HLCIT_Ward\",\n",
    "                         11,#\"CBS_Ward\",\n",
    "                         12,#\"wgs84_x\",\n",
    "                         13,#\"wgs84_y\"\n",
    "                     ],\n",
    "                     header=0,\n",
    "                     names=[\n",
    "                         \"ix\",\n",
    "                         #\"OBJECTID\",\n",
    "                         \"VALUE_PTS\",\n",
    "                         #\"PALIKA\",\n",
    "                         #\"DISTRICT\",\n",
    "                         #\"GAPA_NAPA\",\n",
    "                         #\"GN_TYPE\",\n",
    "                         #\"PROVINCE\",\n",
    "                         \"CBS_ID\",\n",
    "                         #\"HLCIT_ID\",\n",
    "                         #\"HLCIT_Ward\",\n",
    "                         \"CBS_Ward\",\n",
    "                         \"x\",\n",
    "                         \"y\"\n",
    "                     ],\n",
    "                     dtype = {\n",
    "                         \"ix\": int, \n",
    "                         #\"OBJECTID\": int,\n",
    "                         \"VALUE_PTS\": float, \n",
    "                         #\"PALIKA\": str,\n",
    "                         #\"DISTRICT\": str,\n",
    "                         #\"GAPA_NAPA\":str, \n",
    "                         #\"GN_TYPE\": str, \n",
    "                         #\"PROVINCE\": str,\n",
    "                         \"CBS_ID\":str,  \n",
    "                         #\"HLCIT_ID\":str,\n",
    "                         #\"HLCIT_Ward\":str,\n",
    "                         \"CBS_Ward\": str,\n",
    "                         \"x\": float, \n",
    "                         \"y\": float \n",
    "                     },\n",
    "                     na_values = ' ',\n",
    "                     #blocksize='25mb'#'100mb'\n",
    "                ).set_index('ix', drop=True, sorted=True)\n",
    "\n",
    "points['CBS_Ward'] = points['CBS_Ward'].fillna('0')\n",
    "points['CBS_ID'] = points['CBS_ID'].fillna('0')\n",
    "points['CBS_Ward'] = points['CBS_Ward'].astype(float).astype(int)\n",
    "points['CBS_ID'] = points['CBS_ID'].astype(float).astype(int)\n",
    "points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp subset points\n",
    "#points_keep = [\n",
    "#    10609,\n",
    "#    31101,\n",
    "#    60703,\n",
    "#    40303,\n",
    "#    40301\n",
    "#]\n",
    "\n",
    "#points = points[points['CBS_ID'].isin(points_keep)]\n",
    "#out_path_mod = \"_subset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove low pop densities\n",
    "points = points[points['VALUE_PTS'] > DENSITY_THRESHOLD]\n",
    "#points = points.repartition(npartitions=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persist raster: dry_allroads_1993 at s3://data-fully-public-bucket/nepal/cog/dry_allroads_1993_20210516_WGS84_COG.tif\n",
      "Persist raster: dry_allroads_2001 at s3://data-fully-public-bucket/nepal/cog/dry_allroads_2001_20210517_WGS84_COG.tif\n",
      "Persist raster: dry_allroads_2011 at s3://data-fully-public-bucket/nepal/cog/dry_allroads_2011_20210516_WGS84_COG.tif\n",
      "Persist raster: dry_allroads_2021 at s3://data-fully-public-bucket/nepal/cog/dry_allroads_2021_20210517_WGS84_COG.tif\n",
      "Persist raster: dry_highways_1993 at s3://data-fully-public-bucket/nepal/cog/dry_highways_1993_20210516_WGS84_COG.tif\n",
      "Persist raster: dry_highways_2001 at s3://data-fully-public-bucket/nepal/cog/dry_highways_2001_20210517_WGS84_COG.tif\n",
      "Persist raster: dry_highways_2011 at s3://data-fully-public-bucket/nepal/cog/dry_highways_2011_20210516_WGS84_COG.tif\n",
      "Persist raster: dry_highways_2021 at s3://data-fully-public-bucket/nepal/cog/dry_highways_2021_20210517_WGS84_COG.tif\n",
      "Persist raster: dry_urban_centers_1993 at s3://data-fully-public-bucket/nepal/cog/dry_urban_centers_1993_20210516_WGS84_COG.tif\n",
      "Persist raster: dry_urban_centers_2001 at s3://data-fully-public-bucket/nepal/cog/dry_urban_centers_2001_20210517_WGS84_COG.tif\n",
      "Persist raster: dry_urban_centers_2011 at s3://data-fully-public-bucket/nepal/cog/dry_urban_centers_2011_20210516_WGS84_COG.tif\n",
      "Persist raster: dry_urban_centers_2021 at s3://data-fully-public-bucket/nepal/cog/dry_urban_centers_2021_20210517_WGS84_COG.tif\n",
      "Persist raster: msn_allroads_1993 at s3://data-fully-public-bucket/nepal/cog/msn_allroads_1993_20210516_WGS84_COG.tif\n",
      "Persist raster: msn_allroads_2001 at s3://data-fully-public-bucket/nepal/cog/msn_allroads_2001_20210517_WGS84_COG.tif\n",
      "Persist raster: msn_allroads_2011 at s3://data-fully-public-bucket/nepal/cog/msn_allroads_2011_20210517_WGS84_COG.tif\n",
      "Persist raster: msn_allroads_2021 at s3://data-fully-public-bucket/nepal/cog/msn_allroads_2021_20210517_WGS84_COG.tif\n",
      "Persist raster: msn_highways_1993 at s3://data-fully-public-bucket/nepal/cog/msn_highways_1993_20210516_WGS84_COG.tif\n",
      "Persist raster: msn_highways_2001 at s3://data-fully-public-bucket/nepal/cog/msn_highways_2001_20210517_WGS84_COG.tif\n",
      "Persist raster: msn_highways_2011 at s3://data-fully-public-bucket/nepal/cog/msn_highways_2011_20210517_WGS84_COG.tif\n",
      "Persist raster: msn_highways_2021 at s3://data-fully-public-bucket/nepal/cog/msn_highways_2021_20210517_WGS84_COG.tif\n",
      "Persist raster: msn_urban_centers_2011 at s3://data-fully-public-bucket/nepal/cog/msn_urban_centers_2011_20210517_WGS84_COG.tif\n",
      "Persist raster: msn_urban_centers_1993 at s3://data-fully-public-bucket/nepal/cog/msn_urban_centers_1993_20210516_WGS84_COG.tif\n",
      "Persist raster: msn_urban_centers_2001 at s3://data-fully-public-bucket/nepal/cog/msn_urban_centers_2001_20210517_WGS84_COG.tif\n",
      "Persist raster: msn_urban_centers_2021 at s3://data-fully-public-bucket/nepal/cog/msn_urban_centers_2021_20210517_WGS84_COG.tif\n"
     ]
    }
   ],
   "source": [
    "loaded_rasters = {}\n",
    "for key in rasters:\n",
    "    print(f\"Persist raster: {key} at {COG_BASE}/{rasters[key]}\")\n",
    "    raster = xr.open_rasterio(f\"{COG_BASE}/{rasters[key]}\", \n",
    "                                           chunks = (4, \"auto\", -1),\n",
    "                                           parse_coordinates=True)\n",
    "                                           #lock = True)\n",
    "    loaded_rasters[key] = raster\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rasters_ds = (\n",
    "    xr.Dataset(loaded_rasters)\n",
    "    .sel(band=1)\n",
    "    .map(lambda arr: arr.where(arr != arr.nodatavals[0]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pixels = pixel_values_at_points(rasters_ds, points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep all points where dry_allroads_2021_hrs < FRICTION_Threshold\n",
    "df_pixels = df_pixels[df_pixels.dry_allroads_2021 < FRICTION_THRESHOLD]\n",
    "df_pixels = df_pixels.loc[:,~df_pixels.columns.duplicated()]\n",
    "df_pixels = df_pixels.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Pops by CBS_ID\n",
    "adm3_pop = df_pixels.groupby('CBS_ID')['VALUE_PTS'].sum().to_frame(\"adm3_pop\")\n",
    "\n",
    "# Get Pops by CBS Ward\n",
    "adm4_pop = df_pixels.groupby('CBS_Ward')['VALUE_PTS'].sum().to_frame(\"adm4_pop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the Pops into Ref DF\n",
    "df_pixels = dd.merge(df_pixels, adm3_pop, how='left', left_on=\"CBS_ID\", right_index=True)\n",
    "df_pixels = dd.merge(df_pixels, adm4_pop, how = 'left', left_on=\"CBS_Ward\", right_index=True)\n",
    "\n",
    "\n",
    "#points = points.persist()\n",
    "\n",
    "df_pixels = df_pixels.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Weights for each admin area\n",
    "df_pixels['wt_adm_3'] = (df_pixels['VALUE_PTS'] / df_pixels['adm3_pop'])\n",
    "df_pixels['wt_adm_4'] = (df_pixels['VALUE_PTS'] / df_pixels['adm4_pop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm3_avg_cols = []\n",
    "adm4_avg_cols = []\n",
    "for rkey in rasters:\n",
    "    hrs_col = f\"{rkey}\"\n",
    "    avg_col_adm_3 = f\"{rkey}_avt_adm3\"\n",
    "    avg_col_adm_4 = f\"{rkey}_avt_adm4\"\n",
    "    df_pixels[avg_col_adm_3] = df_pixels[hrs_col] * df_pixels['wt_adm_3']\n",
    "    df_pixels[avg_col_adm_4] = df_pixels[hrs_col] * df_pixels['wt_adm_4']\n",
    "    \n",
    "    adm3_avg_cols.append(avg_col_adm_3)\n",
    "    adm4_avg_cols.append(avg_col_adm_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pixels = df_pixels.persist()\n",
    "#df_pixels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dry_allroads_1993_avt_adm3,dry_allroads_2001_avt_adm3,dry_allroads_2011_avt_adm3,dry_allroads_2021_avt_adm3,dry_highways_1993_avt_adm3,dry_highways_2001_avt_adm3,dry_highways_2011_avt_adm3,dry_highways_2021_avt_adm3,dry_urban_centers_1993_avt_adm3,dry_urban_centers_2001_avt_adm3,dry_urban_centers_2011_avt_adm3,dry_urban_centers_2021_avt_adm3,msn_allroads_1993_avt_adm3,msn_allroads_2001_avt_adm3,msn_allroads_2011_avt_adm3,msn_allroads_2021_avt_adm3,msn_highways_1993_avt_adm3,msn_highways_2001_avt_adm3,msn_highways_2011_avt_adm3,msn_highways_2021_avt_adm3,msn_urban_centers_2011_avt_adm3,msn_urban_centers_1993_avt_adm3,msn_urban_centers_2001_avt_adm3,msn_urban_centers_2021_avt_adm3\n",
      "dry_allroads_1993_avt_adm4,dry_allroads_2001_avt_adm4,dry_allroads_2011_avt_adm4,dry_allroads_2021_avt_adm4,dry_highways_1993_avt_adm4,dry_highways_2001_avt_adm4,dry_highways_2011_avt_adm4,dry_highways_2021_avt_adm4,dry_urban_centers_1993_avt_adm4,dry_urban_centers_2001_avt_adm4,dry_urban_centers_2011_avt_adm4,dry_urban_centers_2021_avt_adm4,msn_allroads_1993_avt_adm4,msn_allroads_2001_avt_adm4,msn_allroads_2011_avt_adm4,msn_allroads_2021_avt_adm4,msn_highways_1993_avt_adm4,msn_highways_2001_avt_adm4,msn_highways_2011_avt_adm4,msn_highways_2021_avt_adm4,msn_urban_centers_2011_avt_adm4,msn_urban_centers_1993_avt_adm4,msn_urban_centers_2001_avt_adm4,msn_urban_centers_2021_avt_adm4\n"
     ]
    }
   ],
   "source": [
    "print(\",\".join(adm3_avg_cols))\n",
    "print(\",\".join(adm4_avg_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%time df_pixels.to_csv(f\"s3://data-fully-public-bucket/nepal/output/19m/df_pixels_raw{out_path_mod}.csv\", single_file = True, storage_options=STORAGE_OPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgu = df_pixels.groupby(['CBS_ID'])[adm3_avg_cols].sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ward = df_pixels.groupby(['CBS_Ward'])[adm4_avg_cols].sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgu_output = f\"s3://data-fully-public-bucket/nepal/output/19m/lgu{out_path_mod}.csv\"\n",
    "ward_output = f\"s3://data-fully-public-bucket/nepal/output/19m/ward{out_path_mod}.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://data-fully-public-bucket/nepal/output/19m/lgu.csv\n"
     ]
    }
   ],
   "source": [
    "print(lgu_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrmaksim/Code/rbanick_grav/venv/lib/python3.8/site-packages/dask/dataframe/io/csv.py:814: UserWarning: Appending data to a network storage system may not work.\n",
      "  warn(\"Appending data to a network storage system may not work.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 520 ms, sys: 34.3 ms, total: 555 ms\n",
      "Wall time: 3min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data-fully-public-bucket/nepal/output/19m/lgu.csv']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time lgu.to_csv(lgu_output, single_file = True, storage_options=STORAGE_OPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 137 ms, sys: 0 ns, total: 137 ms\n",
      "Wall time: 4.18 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data-fully-public-bucket/nepal/output/19m/ward.csv']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "asyncio.exceptions.CancelledError\n"
     ]
    }
   ],
   "source": [
    "%time ward.to_csv(ward_output, single_file = True, storage_options=STORAGE_OPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import xarray as xr\n",
    "data = xr.DataArray(\n",
    "    da.random.random((256, 512), chunks=100),\n",
    "    coords=[(\"y\", np.linspace(0, 1, 256)), (\"x\", np.linspace(0, 1, 512))],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dask.datasets.timeseries()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_values = pixel_values_at_points(data, df)\n",
    "df_with_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#points.head()\n",
    "chunk_list = []\n",
    "\n",
    "for chunk_df in points:\n",
    " \n",
    "    ind_x = xr.DataArray(chunk_df.x.values, dims=\"unique_index\")\n",
    "    ind_y = xr.DataArray(chunk_df.y.values, dims=\"unique_index\")\n",
    "    \n",
    "    # TODO - this should be parallel somehow\n",
    "    for rkey in loaded_rasters:\n",
    "        print(f\"Extracting from {rkey}\")\n",
    "        rds = loaded_rasters[rkey]\n",
    "        fill_val = rds.attrs['_FillValue']\n",
    "        result = rds.sel(band=1, x = ind_x, y = ind_y, method='nearest').values\n",
    "        chunk_df[f\"{rkey}_hrs\"] = result\n",
    "        chunk_df[chunk_df[f\"{rkey}_hrs\"] == fill_val] = None\n",
    "    \n",
    "    chunk_list.append(chunk_df)\n",
    "    \n",
    "p2 = pd.concat(chunk_list)\n",
    "p2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "for chunk_df in points:\n",
    "    print('e')\n",
    "    ind_x = xr.DataArray(chunk_df.x.values, dims=\"unique_index\")\n",
    "    ind_y = xr.DataArray(chunk_df.y.values, dims=\"unique_index\")\n",
    "    \n",
    "    # TODO - this should be parallel somehow\n",
    "    res = {}\n",
    "    for rkey in loaded_rasters:\n",
    "        print(f\"Extracting from {rkey}\")\n",
    "        rds = loaded_rasters[rkey]\n",
    "        res[rkey] = rds.sel(band=1, x = ind_x, y = ind_y, method='nearest')\n",
    "        \n",
    "    results.append(res)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = []\n",
    "for r in results:\n",
    "    proc.append(r['msn_allroads_2021'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = dask.compute(*proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_rasters = {}\n",
    "#url = \"https://data-fully-public-bucket.s3.amazonaws.com/nepal/cog/dry_allroads_2021_20210517_WGS84_COG.tif\"\n",
    "#url2 = \"https://data-fully-public-bucket.s3.amazonaws.com/nepal/cog/msn_allroads_2021_20210517_WGS84_COG.tif\"\n",
    "#loaded_rasters['dry_uncog'] = rx.open_rasterio(url, \n",
    "#                                       chunks = (4, \"auto\", -1),\n",
    "#                                       masked=False,\n",
    "#                                       parse_coordinates=True, \n",
    "#                                       lock = True).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_values_from_rasters(chunk_df, loaded_rasters, partition_info=None): \n",
    "    print(partition_info)\n",
    "    #If divisions are not known (for instance if the index is not sorted) then you will get None as the division.\n",
    "    ind_x = xr.DataArray(chunk_df.x.values, dims=\"unique_index\")\n",
    "    ind_y = xr.DataArray(chunk_df.y.values, dims=\"unique_index\")\n",
    "    \n",
    "    # TODO - this should be parallel somehow\n",
    "    for rkey in loaded_rasters:\n",
    "        print(f\"Extracting from {rkey}\")\n",
    "        rds = loaded_rasters[rkey]\n",
    "        fill_val = rds.attrs['_FillValue']\n",
    "        result = rds.sel(band=1, x = ind_x, y = ind_y, method='nearest').values\n",
    "        chunk_df[f\"{rkey}_hrs\"] = result\n",
    "        chunk_df[chunk_df[f\"{rkey}_hrs\"] == fill_val] = None\n",
    "    return chunk_df\n",
    "\n",
    "# Build output meta\n",
    "meta = {'VALUE': float, 'CBS_ID': int, 'CBS_Ward': int, 'x': float, 'y': float}\n",
    "for rkey in loaded_rasters:\n",
    "    meta[f\"{rkey}_hrs\"] = float\n",
    "\n",
    "\n",
    "# Promise the calculation\n",
    "# Adding loaded rasters per coiled team suggestion\n",
    "points = points.map_partitions(get_values_from_rasters, loaded_rasters=loaded_rasters,\n",
    "                                 meta=meta)\n",
    "points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgu = points.groupby(['CBS_ID'])[adm3_avg_cols].sum()\n",
    "lgu_output = \"s3://data-fully-public-bucket/nepal/output/500k/lgu.csv\"\n",
    "#lgu_output = \"../data/nepal/lgu_test.csv\"\n",
    "lgu.to_csv(lgu_output, single_file = True, storage_options=STORAGE_OPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ward = points.groupby(['CBS_Ward'])[adm4_avg_cols].sum()\n",
    "#ward_output = \"s3://data-fully-public-bucket/nepal/output/500k/lgu.csv\"\n",
    "#ward_output = \"../data/nepal/ward_test.csv\"\n",
    "#ward.to_csv(ward_output, single_file = True, storage_options=STORAGE_OPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#points.visualize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
