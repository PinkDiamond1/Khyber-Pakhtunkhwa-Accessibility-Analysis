{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, importlib, json\n",
    "import geohash, rasterio\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "from shapely.geometry import box\n",
    "\n",
    "sys.path.append('../../')\n",
    "\n",
    "import infrasap.rasterMisc as rMisc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize hotspots in features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspot_folder = \"/home/wb411133/data/Projects/HeightComparison\"\n",
    "hotspot_files = ['pop_floor.tif'] #,'water_points_hotspots.tif','toilets_hotspots.tif','shops_hotspots.tif']\n",
    "all_res = []\n",
    "\n",
    "for root, dirs, files in os.walk(hotspot_folder):\n",
    "    for f in files:\n",
    "        if f in hotspot_files:\n",
    "            print(os.path.basename(root))\n",
    "            #Document the extent of the analysis\n",
    "            curR = rasterio.open(os.path.join(root, f))\n",
    "            shp = box(*curR.bounds)\n",
    "            all_res.append([os.path.basename(root), shp])\n",
    "            \n",
    "all_extents = pd.DataFrame(all_res, columns=[\"city\",\"geometry\"])\n",
    "all_extents = gpd.GeoDataFrame(all_extents, geometry=\"geometry\", crs=curR.crs)\n",
    "all_extents.to_file(\"%s_extents.shp\" % hotspot_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspot_extents = all_extents.unary_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspot_files = ['pop_floor.tif','water_points_hotspots.tif','toilets_hotspots.tif','shops_hotspots.tif']\n",
    "hotspot_folder = \"/home/wb411133/data/Projects/HeightComparison\"\n",
    "out_folder = \"/home/wb411133/data/Projects/CoVID_hotspots\"\n",
    "\n",
    "for cov_folder in all_folders:\n",
    "    final_file = os.path.join(out_folder, f\"{os.path.basename(cov_folder)}_hotspots.csv\")\n",
    "    if not os.path.exists(final_file):\n",
    "        print(os.path.basename(cov_folder))\n",
    "        fishnets = []\n",
    "        for root, dirs, files in os.walk(cov_folder):\n",
    "            for f in files:\n",
    "                if (\"URBAN\" in f) & (f[-4:] == \".shp\"):\n",
    "                    fishnets.append(os.path.join(root, f))        \n",
    "        try:\n",
    "            del(final)\n",
    "        except:\n",
    "            pass\n",
    "        for f in fishnets:\n",
    "            cur_f = gpd.read_file(f)\n",
    "            if cur_f.unary_union.intersects(hotspot_extents):\n",
    "                sel_h = all_extents.loc[all_extents.intersects(cur_f.unary_union)]\n",
    "                city = sel_h[\"city\"].iloc[0]\n",
    "                print(f'***** {os.path.basename(f)} {cur_f.shape[0]}: {city}')\n",
    "                cur_hotspot_folder = os.path.join(hotspot_folder, city)   \n",
    "                for h in hotspot_files:\n",
    "                    cur_hot = os.path.join(cur_hotspot_folder, h)\n",
    "                    field_name = h.split(\"_\")[0]\n",
    "                    if os.path.exists(cur_hot):\n",
    "                        res = rMisc.zonalStats(cur_f, cur_hot)\n",
    "                        res = pd.DataFrame(res, columns=['SUM', 'MIN', 'MAX', 'MEAN'])\n",
    "                        cur_f[field_name] = (res['MAX'] > 0.29).astype(int)\n",
    "                    else:\n",
    "                        cur_f[field_name] = 0\n",
    "                try:\n",
    "                    final = final.append(cur_f)\n",
    "                except:\n",
    "                    final = cur_f\n",
    "        try:\n",
    "            pd.DataFrame(final).drop(['geometry'], axis=1).to_csv(final_file)       \n",
    "        except:\n",
    "            print(\"DOES NOT INTERSECT HOTSPOTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(final).drop(['geometry'], axis=1).to_csv(final_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_defs = {'pop':'P10','water':'P11','toilets':'P12','shops':'P13'}\n",
    "for f in os.listdir(out_folder):\n",
    "    cur_f = os.path.join(out_folder, f)\n",
    "    curD = pd.read_csv(cur_f, index_col=0)\n",
    "    curD = curD.rename(columns=rename_defs)\n",
    "    curD.to_csv(cur_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check on processing status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_folder = \"/home/wb411133/data/Projects/CoVID\"\n",
    "all_folders = []\n",
    "all_zips = []\n",
    "for thing in os.listdir(covid_folder):\n",
    "    path = os.path.join(covid_folder, thing)\n",
    "    if os.path.isdir(path):\n",
    "        all_folders.append(path)\n",
    "    elif thing[-4:] == \".zip\":\n",
    "        all_zips.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {}\n",
    "bad_files = []\n",
    "for folder in all_folders:\n",
    "    shps = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for f in files:\n",
    "            if f[-4:] == \".shp\":\n",
    "                shps.append(f)\n",
    "            if f[:3] == \"ADM\":\n",
    "                bad_files.append(os.path.join(root, f))\n",
    "    outputs[os.path.basename(folder)] = shps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for DevSeed\n",
    "\n",
    "Need to ensure data follow strict data structure:\n",
    "\n",
    "1. All files delivered as GeoJSON\n",
    "2. Ensure all files have geohash\n",
    "3. Deliver preliminary zonal results as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from infrasap import covid_data_extraction as cov\n",
    "importlib.reload(cov)\n",
    "    \n",
    "dhs_definitions = '/home/public/Data/PROJECTS/CoVID/DHS/column_definitions.csv'\n",
    "dhs = pd.read_csv(dhs_definitions)\n",
    "column_defs = {}\n",
    "for idx, row in dhs.iterrows():\n",
    "    column_defs['%s_SUM' % row['Column_name']] = \"%s_SUM\" % row['Tight_Name'] #'Total %s' % row['Definition']\n",
    "    column_defs['%s_MEAN' % row['Column_name']] = \"%s_MEAN\" % row['Tight_Name'] #'Mean density %s' % row['Definition']    \n",
    "\n",
    "class covid_result(object):\n",
    "    def __init__(self, country, files, cur_folder, dhs_column_defs):\n",
    "        self.dhs_column_defs = dhs_column_defs\n",
    "        self.iso3 = country\n",
    "        self.files = files\n",
    "        self.cur_folder = cur_folder\n",
    "        self.LC_cols = ['LC_%s' % x for x in [11,14,20,30,40,50,60,70,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230]]\n",
    "        self.final_folder = os.path.join(cur_folder, \"FINAL_GEOMS\")\n",
    "        self.final_stats_folder = os.path.join(cur_folder, \"FINAL_STATS\")\n",
    "        self.bad_cols = ['R10_SUM',\"P1_SUM\",\"P2_SUM\",\"Shape_Leng\",\"Shape_Area\"]\n",
    "        self.ready = True\n",
    "        \n",
    "        if not os.path.exists(self.final_folder):\n",
    "            os.makedirs(self.final_folder)\n",
    "        if not os.path.exists(self.final_stats_folder):\n",
    "            os.makedirs(self.final_stats_folder)\n",
    "    \n",
    "    \n",
    "    def check_zonal(self):\n",
    "        csv_files = []\n",
    "        for root, dirs, files in os.walk(self.cur_folder):\n",
    "            for f in files:\n",
    "                if f[-4:] == \".csv\" and not \"FINAL_STATS\" in root:\n",
    "                    csv_files.append(os.path.join(root, f))\n",
    "        self.csv_files = csv_files\n",
    "        \n",
    "        for csv_file in csv_files:\n",
    "            cur_res = pd.read_csv(csv_file, index_col=0)\n",
    "            geom_file = os.path.basename(csv_file).replace(\".csv\",\"\").replace(\"_zonal\",\"\").replace(\"_BASE\",\"\").replace(\"_DHS\",\"\") + \".geojson\"\n",
    "            geom_file = os.path.join(self.final_folder, geom_file)\n",
    "            if os.path.exists(geom_file):\n",
    "                raw_data = gpd.read_file(geom_file)\n",
    "                pk_column = \"geohash\"\n",
    "                if os.path.basename(csv_file)[:3] == \"adm\":\n",
    "                    pk_column = f\"WB_ADM{os.path.basename(csv_file)[3:4]}_CO\"\n",
    "                if \"BASE\" in csv_file:\n",
    "                    orig_columns = list(cur_res.columns)\n",
    "                    orig_columns[-len(self.LC_cols):] = self.LC_cols\n",
    "                    cur_res.columns = orig_columns\n",
    "                if \"_DHS\" in csv_file:\n",
    "                    cur_res.rename(columns = self.dhs_column_defs, inplace=True)\n",
    "                cur_res['geom_key'] = raw_data[pk_column]\n",
    "                cur_res.to_csv(os.path.join(self.final_stats_folder, os.path.basename(csv_file)))\n",
    "        return(self.csv_files)\n",
    "            \n",
    "\n",
    "    def check_for_fishnets(self):\n",
    "        self.fishnets = False\n",
    "        for f in self.files:\n",
    "            if \"URBAN_\" in f:\n",
    "                self.fishnets = True\n",
    "                return(self.fishnets)\n",
    "        return(self.fishnets)\n",
    "        \n",
    "    def write_tiff_zip(self, zip_file):\n",
    "        zipf = zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED)        \n",
    "        tiff_files = [\"LC.tif\", \"WP_2020_1km.tif\", \"WP_2020_1km_urban_pop.tif\", \"WP2020_vulnerability_map.tif\"]\n",
    "        for t in tiff_files:\n",
    "            cur_t = os.path.join(self.cur_folder, t)\n",
    "            zipf.write(cur_t)            \n",
    "    \n",
    "    def write_geom_data(self):\n",
    "        for f in self.files:\n",
    "            inD = gpd.read_file(f)\n",
    "            inD = inD.to_crs({'init':'epsg:4326'})\n",
    "            if not 'geohash' in inD.columns:\n",
    "                try:\n",
    "                    inD['geohash'] = inD['geometry'].apply(lambda x: geohash.encode(x.centroid.y, x.centroid.x))\n",
    "                except:\n",
    "                    raise(ValueError(f))\n",
    "            for col in self.bad_cols:\n",
    "                if col in inD.columns:\n",
    "                    inD.drop([col],axis=1,inplace=True)\n",
    "            inD.to_file(os.path.join(self.final_folder, os.path.basename(f).replace(\".shp\",\".geojson\")), driver=\"GeoJSON\")\n",
    "      \n",
    "    def zip_folder(self, folder, out_file):\n",
    "        # ziph is zipfile handle\n",
    "        zipf = zipfile.ZipFile(out_file, 'w', zipfile.ZIP_DEFLATED)\n",
    "        for root, dirs, files in os.walk(folder):\n",
    "            for file in files:\n",
    "                zipf.write(os.path.join(root, file))\n",
    "                \n",
    "    def check_for_zip(self):\n",
    "        stats_files = os.listdir(self.final_stats_folder)\n",
    "        # list all geojson files in FINAL_GEOMS\n",
    "        self.ready = True\n",
    "        for g_file in os.listdir(self.final_folder):\n",
    "            stats_file_base = g_file.replace(\".geojson\", \"_zonal_BASE.csv\")\n",
    "            #stats_file_dhs = g_file.replace(\".geojson\", \"_zonal_DHS.csv\")\n",
    "            if (not stats_file_base in stats_files):# or (not stats_file_dhs in stats_files):                \n",
    "                self.ready = False\n",
    "        return(self.ready)\n",
    "\n",
    "def process_all(iso3, files, out_z_folder):\n",
    "    print(iso3)\n",
    "    final_geometry_zip = os.path.join(out_z_folder, \"%s_GEOMETRY.zip\" % iso3)\n",
    "    final_stats_zip = os.path.join(out_z_folder, \"%s_STATS.zip\" % iso3)\n",
    "    xx = covid_result(iso3, files, os.path.join(covid_folder, iso3), column_defs)\n",
    "    final_tiff_zip = os.path.join(out_zip_folder, \"%s_tiffs.zip\" % iso3)\n",
    "    if not os.path.exists(final_tiff_zip):\n",
    "        xx.write_tiff_zip(final_tiff_zip)        \n",
    "    if not os.path.exists(final_geometry_zip) or not os.path.exists(final_stats_zip):\n",
    "        fishnets_exist = xx.check_for_fishnets()\n",
    "        if not fishnets_exist:\n",
    "            print(\"Creating fishnet for %s\" % iso3)\n",
    "            extent_file = os.path.join(covid_folder, iso3, \"urban_areas_hd.shp\")\n",
    "            prefix = \"HD_URBAN\"\n",
    "            out_folder = os.path.join(covid_folder, iso3, \"hd_urban_fishnets\")\n",
    "            if not os.path.exists(extent_file):\n",
    "                extent_file = os.path.join(covid_folder, iso3, \"urban_areas.shp\")\n",
    "                prefix = \"URBAN\"\n",
    "                out_folder = os.path.join(covid_folder, iso3, \"urban_fishnets\")\n",
    "            if os.path.exists(extent_file):\n",
    "                if not os.path.exists(out_folder):\n",
    "                    os.makedirs(out_folder)\n",
    "                cov.create_fishnet(extent_file, out_folder, prefix)\n",
    "            else:\n",
    "                print(\"ERROR with %s\" % iso3)\n",
    "        xx.write_geom_data()\n",
    "        xx.zip_folder(xx.final_folder, final_geometry_zip)            \n",
    "        res = xx.check_zonal()\n",
    "        if xx.check_for_zip():\n",
    "            xx.zip_folder(xx.final_stats_folder, final_stats_zip)\n",
    "        else:\n",
    "            print(\"%s is not ready\" % iso3)\n",
    "    return(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_zip_folder = \"/home/wb411133/data/Projects/CoVID_FINAL\"\n",
    "covid_folder = \"/home/wb411133/data/Projects/CoVID\"\n",
    "outputs = {}\n",
    "bad_files = []\n",
    "for folder in all_folders:\n",
    "    shps = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for f in files:\n",
    "            if f[-4:] == \".shp\":\n",
    "                shps.append(os.path.join(root, f))\n",
    "            if f[:3] == \"ADM\":\n",
    "                bad_files.append(os.path.join(root, f))\n",
    "    outputs[os.path.basename(folder)] = shps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EGY\n"
     ]
    }
   ],
   "source": [
    "iso3 = 'EGY'\n",
    "files = outputs[iso3]\n",
    "res = process_all(iso3, files, out_zip_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VNM\n",
      "ARG\n",
      "PAK\n",
      "ZAF\n",
      "COL\n",
      "ZWE\n",
      "MNG\n",
      "SLE\n",
      "CPV\n",
      "KEN\n",
      "GHA\n",
      "AFG\n",
      "YEM\n",
      "ECU\n",
      "PRY\n",
      "MRT\n",
      "MDV\n",
      "Creating fishnet for MDV\n",
      "ERROR with MDV\n",
      "KGZ\n",
      "HTI\n",
      "DJI\n",
      "KHM\n",
      "TJK\n",
      "GMB\n",
      "LKA\n",
      "SEN\n",
      "STP\n",
      "SLV\n",
      "VEN\n",
      "MLI\n",
      "RWA\n",
      "BOL\n",
      "TZA\n",
      "MAR\n",
      "IND\n",
      "IDN\n",
      "SDN\n",
      "AGO\n",
      "BEN\n",
      "BWA\n",
      "BFA\n",
      "BDI\n",
      "CMR\n",
      "CAF\n",
      "TCD\n",
      "COM\n",
      "COG\n",
      "CIV\n",
      "COD\n",
      "SSD\n",
      "ERI\n",
      "ETH\n",
      "GAB\n",
      "GNB\n",
      "GIN\n",
      "LSO\n",
      "LBR\n",
      "MDG\n",
      "MWI\n",
      "MUS\n",
      "MOZ\n",
      "NAM\n",
      "NER\n",
      "NGA\n",
      "SYC\n",
      "SOM\n",
      "SWZ\n",
      "TGO\n",
      "UGA\n",
      "ZMB\n",
      "LCA\n",
      "PHL\n",
      "GTM\n",
      "BGD\n",
      "BRA\n",
      "MEX\n",
      "EGY\n",
      "UKR\n",
      "PER\n",
      "LAO\n",
      "PSE\n",
      "NPL\n",
      "PNG\n",
      "Creating fishnet for PNG\n",
      "ERROR with PNG\n",
      "DZA\n",
      "BLR\n",
      "BTN\n",
      "Creating fishnet for BTN\n",
      "ERROR with BTN\n",
      "BIH\n",
      "NIC\n",
      "FJI\n",
      "GEO\n",
      "HND\n",
      "JOR\n",
      "MHL\n",
      "Creating fishnet for MHL\n",
      "ERROR with MHL\n",
      "MDA\n",
      "MMR\n",
      "MKD\n",
      "PAN\n",
      "WSM\n",
      "Creating fishnet for WSM\n",
      "ERROR with WSM\n",
      "SLB\n",
      "TUN\n",
      "TUR\n",
      "URY\n",
      "UZB\n",
      "ALB\n",
      "HRV\n",
      "IRN\n",
      "SRB\n",
      "TTO\n",
      "ATG\n",
      "Creating fishnet for ATG\n",
      "ERROR with ATG\n",
      "CHN\n",
      "IRQ\n"
     ]
    }
   ],
   "source": [
    "all_bad = []\n",
    "for iso3, files in outputs.items():\n",
    "    res = process_all(iso3, files, out_zip_folder)\n",
    "    if not res.ready:\n",
    "        all_bad.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in all_bad:\n",
    "    print(\"'%s'\" % x.iso3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_aws = \"/home/wb411133/data/Projects/CoVID_FINAL/AWS_FILES.txt\"\n",
    "uploaded = []\n",
    "with open(in_aws, 'r') as in_file:\n",
    "    for line in in_file:\n",
    "        if \"zip\" in line:\n",
    "            uploaded.append(line.split(\" \")[-1].replace(\"\\n\",\"\"))\n",
    "for f in os.listdir(out_zip_folder):\n",
    "    if not f in uploaded and \"zip\" in f:\n",
    "        print(f\"aws s3 cp {f} s3://covid-wb/data/{f} --profile covid\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso3 = 'IDN'\n",
    "files = outputs[iso3]\n",
    "\n",
    "final_geometry_zip = os.path.join(out_zip_folder, \"%s_GEOMETRY.zip\" % iso3)\n",
    "final_stats_zip = os.path.join(out_zip_folder, \"%s_STATS.zip\" % iso3)\n",
    "if not os.path.exists(final_geometry_zip) or not os.path.exists(final_stats_zip):\n",
    "    xx = covid_result(iso3, files, os.path.join(covid_folder, iso3), column_defs)\n",
    "    fishnets_exist = xx.check_for_fishnets()\n",
    "    if not fishnets_exist:\n",
    "        print(\"Creating fishnet for %s\" % iso3)\n",
    "        extent_file = os.path.join(covid_folder, iso3, \"urban_areas_hd.shp\")\n",
    "        prefix = \"HD_URBAN\"\n",
    "        out_folder = os.path.join(covid_folder, iso3, \"hd_urban_fishnets\")\n",
    "        if not os.path.exists(extent_file):\n",
    "            extent_file = os.path.join(covid_folder, iso3, \"urban_areas.shp\")\n",
    "            prefix = \"URBAN\"\n",
    "            out_folder = os.path.join(covid_folder, iso3, \"urban_fishnets\")\n",
    "        if os.path.exists(extent_file):\n",
    "            if not os.path.exists(out_folder):\n",
    "                os.makedirs(out_folder)\n",
    "            cov.create_fishnet(extent_file, out_folder, prefix)\n",
    "        else:\n",
    "            print(\"ERROR with %s\" % iso3)\n",
    "    xx.write_geom_data()\n",
    "    res = xx.check_zonal()\n",
    "    if len(res)/len(xx.files) >= 2:\n",
    "        xx.zip_folder(xx.final_folder, final_geometry_zip)\n",
    "        xx.zip_folder(xx.final_stats_folder, final_stats_zip)\n",
    "    else:\n",
    "        print(\"%s is not ready\" % iso3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(xx.files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update HNP JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhs_definitions = '/home/public/Data/PROJECTS/CoVID/DHS/column_definitions.csv'\n",
    "dhs = pd.read_csv(dhs_definitions)\n",
    "column_defs = {}\n",
    "for idx, row in dhs.iterrows():\n",
    "    column_defs['%s_SUM' % row['Column_name']] = \"%s_SUM\" % row['Tight_Name'] #'Total %s' % row['Definition']\n",
    "    column_defs['%s_MEAN' % row['Column_name']] = \"%s_MEAN\" % row['Tight_Name'] #'Mean density %s' % row['Definition']    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"RiskSchema.json\") as f:\n",
    "    inJ = json.load(f)\n",
    "    \n",
    "inJ['hnp_indicators']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in dhs.iterrows():\n",
    "    cur_def = {'Name': '%s' % row['Definition'],\n",
    "      'file_link': 'csv',\n",
    "      'source': ['DHS'],\n",
    "      'scale': ['fishnet', 'urban_hd', 'urban', 'adm2', 'adm1', 'adm0'],\n",
    "      'cite': ['_FORMAL_CITATION_HARVARD_'],\n",
    "      'vars': ['_PROXY_VARIABLE_'],\n",
    "      'access': 'Public',\n",
    "      'description': 'DHS variables are collected at their delivered administrative divisions and multiplied through the WorldPop gridded population data to get pixel level DHS numbers, which are then aggregated through zonal stats'}\n",
    "    inJ['hnp_indicators'][row['Tight_Name']] = cur_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inJ['hnp_indicators']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"RiskSchema.json\", 'w') as outJ:\n",
    "    json.dump(inJ, outJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (geog)",
   "language": "python",
   "name": "geog"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
